{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2749270c",
   "metadata": {},
   "source": [
    "# **Data Ingestion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ac9f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65377b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Authentication successful!\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary library\n",
    "import tweepy\n",
    "import json\n",
    "\n",
    "#API Bearer token for 'X'\n",
    "bearer_token = \"AAAAAAAAAAAAAAAAAAAAAJtS4QEAAAAAGsySCqbfl%2BfNAv4%2FGlAk9p%2BlS%2FU%3DnESXub5jL0VmT7vtG65GgVstdgLwbcpxvzTicB45V2KL8pzzwh\"\n",
    "\n",
    "# Client object to interact with the API\n",
    "client = tweepy.Client(bearer_token)\n",
    "\n",
    "print(\"‚úÖ Authentication successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43c10c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gempa 5,2 M Guncang Simeulue, Tidak Berpotensi Tsunami, BMKG Imbau Warga Tetap Tenang\n",
      "\n",
      "Gempa dengan kekuatan Magnitudo 5,2 kembali mengguncang wilayah kepulauan Aceh tepatnya di Kota Sinabang, Kabupaten Simeulue, Selasa (23/9/2025).\n",
      "\n",
      "#Gempa #GempaDiAceh\n",
      "\n",
      "https://t.co/uHuKzhscn4\n",
      "---\n",
      "Setiap sudut Museum Tsunami Aceh menyimpan jejak luka dan harapan.\n",
      "Bukan sekadar bangunan, tapi simbol kekuatan dan ingatan.\n",
      "Arsitekturnya berbicara tentang kehilangan, perjuangan, dan bangkit kembali. #museumtsunami https://t.co/SPYJbSO9aH\n",
      "---\n",
      "@tsunami_lonely YOONGI üò≠ He really posted üíú\n",
      "---\n",
      "walaupun melewati badai ombak tsunami dulu ges gpp aku bercaya üòî\n",
      "https://t.co/egq5jVgrUL\n",
      "---\n",
      "Guten Morgen! ‚òïÔ∏èüíô\n",
      "\n",
      "So darf ein Dienstag gerne starten. ‚òùÔ∏èüòé\n",
      "\n",
      "Der Wind dreht sich! ‚ö†Ô∏è Der blaue Tsunami üåä trifft langsam aber sicher das Land‚Ä¶ Wir sind n√§her am Ziel, als wir denken. üòÆ‚Äçüí®üî• https://t.co/cHdCpmBEn8\n",
      "---\n",
      "Kjjj ganamos con media reserva, Funes Mori, y Alario hizo medio gol, ma√±ana lo m√°s tranquilo es un tsunami.\n",
      "---\n",
      "@Alphafox78 Well, getting into my tsunami pod bunker. Hold my breath and then pop out on the other side of the flood. Easy Peasy\n",
      "---\n",
      "@humoorrer @vasilyaarkhipov @Theunk13 IAF ops achievements: Air superiority in 1965/1971 Indo-Pak wars (e.g., 1971 enabled Bangladesh creation); Kargil strikes (1999); Balakot airstrikes (2019); UN Congo peacekeeping (1960s); Siachen airlifts (1984); disaster relief like 2004 tsunami.\n",
      "\n",
      "PLAAF ops achievements: Korean\n",
      "---\n",
      "@algi_zaki61369 @bankrbot Jacking into the matrix to ride OVPP's digital tsunami straight to the moon.\n",
      "\n",
      "OVPP deployed to: 0x4A829Dc75422c316c7Ef4aBa9f608A079C85C170\n",
      "\n",
      "View on: https://t.co/8wT1HO8dum\n",
      "---\n",
      "@brunettecannons \"After I finish tasting, then I want to start deep plumbing and digging  and laying down pipe into your abyss while touching your Cervical womb driving it mad insane while you beg me to send my Tsunami all in you\"!!!!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Define a simple search query\n",
    "# The '-is:retweet' part is useful to avoid duplicates and get original posts\n",
    "query = \"tsunami -is:retweet\"\n",
    "\n",
    "# Use the client to search for recent tweets that match the query. We'll just ask for 10 tweets to start with\n",
    "response = client.search_recent_tweets(query=query, max_results=10)\n",
    "\n",
    "# The data is inside the 'data' attribute of the response\n",
    "tweets = response.data\n",
    "\n",
    "# Let's check if we got anything and print the text of each tweet\n",
    "if tweets:\n",
    "    for tweet in tweets:\n",
    "        print(tweet.text)\n",
    "        print(\"---\")\n",
    "else:\n",
    "    print(\"No tweets found for this query.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea2bd7f",
   "metadata": {},
   "source": [
    "### **Raising the Bar - A More Powerful Search üìà**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5360206e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching with query: (tsunami OR flood OR cyclone OR highwave OR stormsurge OR highwaves OR coastalflood) (India OR Chennai OR Mumbai OR Kerala OR Odisha OR Bengal) -is:retweet lang:en\n"
     ]
    }
   ],
   "source": [
    "# 1. Build a more complex query\n",
    "# Use OR to look for any of these words. Use () to group them.\n",
    "# 'lang:en' filters for English language tweets.\n",
    "keywords = \"(tsunami OR flood OR cyclone OR highwave OR stormsurge OR highwaves OR coastalflood)\"\n",
    "language = \"lang:en\"\n",
    "# Geo-filtering is very limited in the standard API. A good trick is to add place names.\n",
    "# This isn't perfect, but it's a great start.\n",
    "places = \"(India OR Chennai OR Mumbai OR Kerala OR Odisha OR Bengal)\"\n",
    "\n",
    "# Combine everything into one query\n",
    "# The query now looks for any of our keywords AND any of our places.\n",
    "final_query = f\"{keywords} {places} -is:retweet {language}\"\n",
    "\n",
    "print(f\"Searching with query: {final_query}\")\n",
    "\n",
    "# 2. Ask for more data fields. We want to know who posted it, when, and if there's any location info.\n",
    "response = client.search_recent_tweets(\n",
    "    query=final_query,\n",
    "    max_results=50,  # Let's get 100 tweets this time\n",
    "    tweet_fields=[\"created_at\", \"author_id\", \"geo\"], # What info we want about the tweet\n",
    "    user_fields=[\"username\", \"verified\", \"location\"], # What info we want about the user\n",
    "    expansions=[\"author_id\", \"geo.place_id\"] # Tells the API to give us the full user and place objects\n",
    ")\n",
    "\n",
    "# The data is more complex now, so we need a good way to store it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdb2effa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Success! Saved 50 tweets to my_collected_tweets.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# This assumes the 'response' variable from your last run is still in memory\n",
    "tweet_data = []\n",
    "\n",
    "# The response includes the main tweet data and a separate 'includes' dictionary\n",
    "if response.data:\n",
    "    users = {user[\"id\"]: user for user in response.includes['users']}\n",
    "    for tweet in response.data:\n",
    "        author = users[tweet.author_id]\n",
    "        \n",
    "        # Create a clean dictionary for each tweet\n",
    "        tweet_dict = {\n",
    "            \"tweet_id\": tweet.id,\n",
    "            \"created_at\": str(tweet.created_at), # Convert to string for JSON\n",
    "            \"text\": tweet.text,\n",
    "            \"author_username\": author.username,\n",
    "            \"author_verified\": author.verified,\n",
    "            \"author_profile_location\": author.location,\n",
    "            \"tweet_geo\": tweet.geo \n",
    "        }\n",
    "        tweet_data.append(tweet_dict)\n",
    "\n",
    "# Now, save the collected data to a file\n",
    "file_name = \"my_collected_tweets.json\"\n",
    "with open(file_name, \"w\") as f:\n",
    "    json.dump(tweet_data, f, indent=4)\n",
    "\n",
    "print(f\"‚úÖ Success! Saved {len(tweet_data)} tweets to {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4205be",
   "metadata": {},
   "source": [
    "**Model Client**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "503b1232",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DEEPMALYA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Gemini model initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- 1. Setup ---\n",
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# Initialize the Gemini 1.5 Flash model\n",
    "# NOTE: The correct model name is 'gemini-1.5-flash-latest'\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "print(\"‚úÖ Gemini model initialized successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef38d45",
   "metadata": {},
   "source": [
    "**Classifier Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc2295b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. The New, More Sophisticated Batch Classifier ---\n",
    "def classify_tweet_batch_v2(tweet_batch):\n",
    "    \"\"\"\n",
    "    Uses the Gemini LLM with highly specific instructions to identify ACTIVE disaster reports.\n",
    "    \"\"\"\n",
    "    tweets_for_prompt = \"\\n\".join([f'{i+1}. \"{text}\"' for i, text in enumerate(tweet_batch)])\n",
    "\n",
    "    # --- THE REFINED PROMPT ---\n",
    "    prompt = f\"\"\"\n",
    "        You are a real-time situational awareness analyst. Your task is to identify ACTIVE, first-hand reports of ongoing natural disasters or emergencies from a list of texts.\n",
    "\n",
    "        **CRITICAL Instructions:**\n",
    "        1.  Your goal is to find reports from people on the ground, not news or discussions.\n",
    "        2.  A text is only a \"disaster\" if it describes an event happening NOW or very recently.\n",
    "        3.  **EXCLUDE** the following:\n",
    "            - News articles reporting on past or future events.\n",
    "            - General discussions about disaster preparedness.\n",
    "            - Political promises or policy suggestions (e.g., \"we need better cyclone protection\").\n",
    "            - Figurative language or metaphors (e.g., \"a tsunami of fans\").\n",
    "        4.  Provide your response ONLY in a valid JSON array format. Each object must have four keys: \"id\" (the input number), \"is_disaster\" (boolean), \"confidence_score\" (float reflecting severity), and \"report_type\" (string: \"Active Report\", \"News/Discussion\", or \"Figurative/Unrelated\").\n",
    "\n",
    "        **List of Texts to Analyze:**\n",
    "        {tweets_for_prompt}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        time.sleep(5)\n",
    "        response = model.generate_content(prompt)\n",
    "        json_response = json.loads(response.text.strip().replace(\"```json\", \"\").replace(\"```\", \"\"))\n",
    "        return json_response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during batch classification: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef793589",
   "metadata": {},
   "source": [
    "**Batch_Classifier function** - Instead of sending 20 requests to the LLM for 20 times and hitting the limit we send 20 requests as a whole batch one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "742088f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Function to Create Batches ---\n",
    "def create_batches(data, batch_size=20):\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield data[i:i + batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac953b65",
   "metadata": {},
   "source": [
    "**Dummy Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980c2cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Test Cases ---\n",
      "Text: 'A Tsunami of fans flooded the road when MS. Dhoni came out'\n",
      "Is Disaster: False (Score: 0.00)\n",
      "\n",
      "Text: 'Alert: Water logging reported in several parts of Mumbai after heavy rain.'\n",
      "Is Disaster: True (Score: 0.70)\n",
      "\n",
      "Text: 'Just enjoying a beautiful sunset at the beach today!'\n",
      "Is Disaster: False (Score: 0.00)\n",
      "\n",
      "Text: 'BREAKING: Cyclone is expected to make landfall on the east coast tomorrow morning with winds over 150 km/h.'\n",
      "Is Disaster: True (Score: 0.95)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 3. The New Test Function ---\n",
    "def run_test_cases():\n",
    "    print(\"\\n--- Running Test Cases ---\")\n",
    "    \n",
    "    # Create a list of test cases to send as one batch\n",
    "    test_tweets = [\n",
    "        \"A Tsunami of fans flooded the road when MS. Dhoni came out\", # Metaphor\n",
    "        \"Alert: Water logging reported in several parts of Mumbai after heavy rain.\", # Real, low severity\n",
    "        \"Just enjoying a beautiful sunset at the beach today!\", # Not a disaster\n",
    "        \"BREAKING: Cyclone is expected to make landfall on the east coast tomorrow morning with winds over 150 km/h.\" # Real, high severity\n",
    "    ]\n",
    "\n",
    "    # Call the batch function once with the entire list\n",
    "    results = classify_tweet_batch_v2(test_tweets)\n",
    "\n",
    "    if not results:\n",
    "        print(\"Could not get results from the API.\")\n",
    "        return\n",
    "\n",
    "    # Neatly print the result for each test case\n",
    "    for result in results:\n",
    "        # The 'id' from the result corresponds to the position in our test_tweets list\n",
    "        # We subtract 1 because lists are 0-indexed\n",
    "        tweet_index = result['id'] - 1\n",
    "        tweet_text = test_tweets[tweet_index]\n",
    "        is_dis = result['is_disaster']\n",
    "        score = result['confidence_score']\n",
    "        \n",
    "        print(f\"Text: '{tweet_text}'\")\n",
    "        print(f\"Is Disaster: {is_dis} (Score: {score:.2f})\\n\")\n",
    "\n",
    "# --- Run the Test ---\n",
    "run_test_cases()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741807e1",
   "metadata": {},
   "source": [
    "**Augmenting with data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "476fbc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully loaded 50 tweets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches with v2 Prompt:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches with v2 Prompt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:34<00:00, 11.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ‚úÖ Classification Complete! ---\n",
      "Found 12 high-confidence ACTIVE disaster reports.\n",
      "\n",
      "--- Sample of Verified Reports ---\n",
      "                                                 text  confidence_score\n",
      "6   ‚ÄòWorse than Amphan Cyclone‚Äô: Houses flooded, s...               0.9\n",
      "32  @motherrr @manjusipayya @dksardana @Ambarseriy...               0.9\n",
      "44  ‚ÄòWorse than Amphan Cyclone‚Äô: Houses flooded, s...               0.9\n",
      "0   River #Ganga at #Farakka(FF) in #MURSHIDABAD d...               0.8\n",
      "10  BJP is inviting sugg on how  to make Mumbai be...               0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Load Data and Process ---\n",
    "try:\n",
    "    df = pd.read_json('my_collected_tweets.json')\n",
    "    print(f\"\\nSuccessfully loaded {len(df)} tweets.\")\n",
    "    \n",
    "    all_results = []\n",
    "    tweet_list = df['text'].tolist()\n",
    "    \n",
    "    for batch in tqdm(list(create_batches(tweet_list, 20)), desc=\"Processing Batches with v2 Prompt\"):\n",
    "        batch_results = classify_tweet_batch_v2(batch)\n",
    "        all_results.extend(batch_results)\n",
    "\n",
    "    # --- 5. Map Results and Filter ---\n",
    "    if all_results:\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        results_df['index'] = results_df['id'] - 1\n",
    "        df = df.reset_index().merge(results_df, on='index', how='left').drop(columns=['index', 'id'])\n",
    "\n",
    "        # --- THE NEW FILTERING LOGIC ---\n",
    "        # We now filter for \"Active Report\" type for much higher accuracy\n",
    "        verified_disasters_df = df[\n",
    "            (df['is_disaster'] == True) & \n",
    "            (df['report_type'] == 'Active Report')\n",
    "        ].copy()\n",
    "\n",
    "        print(\"\\n--- ‚úÖ Classification Complete! ---\")\n",
    "        print(f\"Found {len(verified_disasters_df)} high-confidence ACTIVE disaster reports.\")\n",
    "        print(\"\\n--- Sample of Verified Reports ---\")\n",
    "        print(verified_disasters_df.sort_values(by='confidence_score', ascending=False)[['text', 'confidence_score']].head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"\\nError: 'my_collected_tweets.json' not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24617baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>confidence_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>‚ÄòWorse than Amphan Cyclone‚Äô: Houses flooded, s...</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>@motherrr @manjusipayya @dksardana @Ambarseriy...</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>‚ÄòWorse than Amphan Cyclone‚Äô: Houses flooded, s...</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>River #Ganga at #Farakka(FF) in #MURSHIDABAD d...</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>BJP is inviting sugg on how  to make Mumbai be...</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>River #Ganga at #CS 97 A GANGA FARAKKA in #MUR...</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>@Matt_Pinner Ofcourse yes! The same flood stor...</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>@RealDrJaneRuby Literally the most synchronize...</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>@BaddieDean @hoar27796 @Codex_India3 thanx to ...</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>‚ÄòWorse than Amphan Cyclone‚Äô: Houses flooded, s...</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>@influenya @CelticAshes As a Black person, thi...</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Cyclone bulletin and #Navratri\\nWeather update...</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  confidence_score\n",
       "6   ‚ÄòWorse than Amphan Cyclone‚Äô: Houses flooded, s...               0.9\n",
       "32  @motherrr @manjusipayya @dksardana @Ambarseriy...               0.9\n",
       "44  ‚ÄòWorse than Amphan Cyclone‚Äô: Houses flooded, s...               0.9\n",
       "0   River #Ganga at #Farakka(FF) in #MURSHIDABAD d...               0.8\n",
       "10  BJP is inviting sugg on how  to make Mumbai be...               0.8\n",
       "3   River #Ganga at #CS 97 A GANGA FARAKKA in #MUR...               0.8\n",
       "19  @Matt_Pinner Ofcourse yes! The same flood stor...               0.7\n",
       "41  @RealDrJaneRuby Literally the most synchronize...               0.6\n",
       "43  @BaddieDean @hoar27796 @Codex_India3 thanx to ...               0.6\n",
       "45  ‚ÄòWorse than Amphan Cyclone‚Äô: Houses flooded, s...               0.6\n",
       "47  @influenya @CelticAshes As a Black person, thi...               0.6\n",
       "49  Cyclone bulletin and #Navratri\\nWeather update...               0.6"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verified_disasters_df.sort_values(by='confidence_score', ascending=False)[['text', 'confidence_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0927e6dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 10)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verified_disasters_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca5326ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>author_username</th>\n",
       "      <th>author_verified</th>\n",
       "      <th>author_profile_location</th>\n",
       "      <th>tweet_geo</th>\n",
       "      <th>is_disaster</th>\n",
       "      <th>confidence_score</th>\n",
       "      <th>report_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1970351141624902010</td>\n",
       "      <td>2025-09-23 04:54:54+00:00</td>\n",
       "      <td>River #Ganga at #Farakka(FF) in #MURSHIDABAD d...</td>\n",
       "      <td>CWCOfficial_FF</td>\n",
       "      <td>False</td>\n",
       "      <td>New Delhi, India</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.8</td>\n",
       "      <td>Active Report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1970350823323627600</td>\n",
       "      <td>2025-09-23 04:53:38+00:00</td>\n",
       "      <td>River #Ganga at #CS 97 A GANGA FARAKKA in #MUR...</td>\n",
       "      <td>CWCOfficial_FF</td>\n",
       "      <td>False</td>\n",
       "      <td>New Delhi, India</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.8</td>\n",
       "      <td>Active Report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1970350658026185072</td>\n",
       "      <td>2025-09-23 04:52:58+00:00</td>\n",
       "      <td>‚ÄòWorse than Amphan Cyclone‚Äô: Houses flooded, s...</td>\n",
       "      <td>akularsnvas</td>\n",
       "      <td>False</td>\n",
       "      <td>Hyderabad, India</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Active Report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1970348990198849929</td>\n",
       "      <td>2025-09-23 04:46:21+00:00</td>\n",
       "      <td>BJP is inviting sugg on how  to make Mumbai be...</td>\n",
       "      <td>Sanjeevdesiguy</td>\n",
       "      <td>False</td>\n",
       "      <td>SPACE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.8</td>\n",
       "      <td>Active Report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1970340495512740159</td>\n",
       "      <td>2025-09-23 04:12:35+00:00</td>\n",
       "      <td>@Matt_Pinner Ofcourse yes! The same flood stor...</td>\n",
       "      <td>VedicBeat</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>Active Report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1970327052403515841</td>\n",
       "      <td>2025-09-23 03:19:10+00:00</td>\n",
       "      <td>@motherrr @manjusipayya @dksardana @Ambarseriy...</td>\n",
       "      <td>joydeepg9</td>\n",
       "      <td>False</td>\n",
       "      <td>Gurgaon, India</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Active Report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1970322667883606430</td>\n",
       "      <td>2025-09-23 03:01:45+00:00</td>\n",
       "      <td>@RealDrJaneRuby Literally the most synchronize...</td>\n",
       "      <td>DJohnT1122</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.6</td>\n",
       "      <td>Active Report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1970320416435585122</td>\n",
       "      <td>2025-09-23 02:52:48+00:00</td>\n",
       "      <td>@BaddieDean @hoar27796 @Codex_India3 thanx to ...</td>\n",
       "      <td>jhonbap78</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.6</td>\n",
       "      <td>Active Report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1970319702703439939</td>\n",
       "      <td>2025-09-23 02:49:58+00:00</td>\n",
       "      <td>‚ÄòWorse than Amphan Cyclone‚Äô: Houses flooded, s...</td>\n",
       "      <td>FinancialXpress</td>\n",
       "      <td>False</td>\n",
       "      <td>India</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Active Report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1970319702703439939</td>\n",
       "      <td>2025-09-23 02:49:58+00:00</td>\n",
       "      <td>‚ÄòWorse than Amphan Cyclone‚Äô: Houses flooded, s...</td>\n",
       "      <td>FinancialXpress</td>\n",
       "      <td>False</td>\n",
       "      <td>India</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.6</td>\n",
       "      <td>Active Report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1970300953640280514</td>\n",
       "      <td>2025-09-23 01:35:28+00:00</td>\n",
       "      <td>@influenya @CelticAshes As a Black person, thi...</td>\n",
       "      <td>LeeH63526936</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.6</td>\n",
       "      <td>Active Report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1970291007855210500</td>\n",
       "      <td>2025-09-23 00:55:57+00:00</td>\n",
       "      <td>Cyclone bulletin and #Navratri\\nWeather update...</td>\n",
       "      <td>cyclo_cast_001</td>\n",
       "      <td>False</td>\n",
       "      <td>Palghar, Maharashtra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.6</td>\n",
       "      <td>Active Report</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tweet_id                created_at  \\\n",
       "0   1970351141624902010 2025-09-23 04:54:54+00:00   \n",
       "3   1970350823323627600 2025-09-23 04:53:38+00:00   \n",
       "6   1970350658026185072 2025-09-23 04:52:58+00:00   \n",
       "10  1970348990198849929 2025-09-23 04:46:21+00:00   \n",
       "19  1970340495512740159 2025-09-23 04:12:35+00:00   \n",
       "32  1970327052403515841 2025-09-23 03:19:10+00:00   \n",
       "41  1970322667883606430 2025-09-23 03:01:45+00:00   \n",
       "43  1970320416435585122 2025-09-23 02:52:48+00:00   \n",
       "44  1970319702703439939 2025-09-23 02:49:58+00:00   \n",
       "45  1970319702703439939 2025-09-23 02:49:58+00:00   \n",
       "47  1970300953640280514 2025-09-23 01:35:28+00:00   \n",
       "49  1970291007855210500 2025-09-23 00:55:57+00:00   \n",
       "\n",
       "                                                 text  author_username  \\\n",
       "0   River #Ganga at #Farakka(FF) in #MURSHIDABAD d...   CWCOfficial_FF   \n",
       "3   River #Ganga at #CS 97 A GANGA FARAKKA in #MUR...   CWCOfficial_FF   \n",
       "6   ‚ÄòWorse than Amphan Cyclone‚Äô: Houses flooded, s...      akularsnvas   \n",
       "10  BJP is inviting sugg on how  to make Mumbai be...   Sanjeevdesiguy   \n",
       "19  @Matt_Pinner Ofcourse yes! The same flood stor...        VedicBeat   \n",
       "32  @motherrr @manjusipayya @dksardana @Ambarseriy...        joydeepg9   \n",
       "41  @RealDrJaneRuby Literally the most synchronize...       DJohnT1122   \n",
       "43  @BaddieDean @hoar27796 @Codex_India3 thanx to ...        jhonbap78   \n",
       "44  ‚ÄòWorse than Amphan Cyclone‚Äô: Houses flooded, s...  FinancialXpress   \n",
       "45  ‚ÄòWorse than Amphan Cyclone‚Äô: Houses flooded, s...  FinancialXpress   \n",
       "47  @influenya @CelticAshes As a Black person, thi...     LeeH63526936   \n",
       "49  Cyclone bulletin and #Navratri\\nWeather update...   cyclo_cast_001   \n",
       "\n",
       "    author_verified author_profile_location  tweet_geo is_disaster  \\\n",
       "0             False        New Delhi, India        NaN        True   \n",
       "3             False        New Delhi, India        NaN        True   \n",
       "6             False        Hyderabad, India        NaN        True   \n",
       "10            False                  SPACE         NaN        True   \n",
       "19            False                    None        NaN        True   \n",
       "32            False          Gurgaon, India        NaN        True   \n",
       "41            False                    None        NaN        True   \n",
       "43            False                    None        NaN        True   \n",
       "44            False                   India        NaN        True   \n",
       "45            False                   India        NaN        True   \n",
       "47            False                    None        NaN        True   \n",
       "49            False   Palghar, Maharashtra         NaN        True   \n",
       "\n",
       "    confidence_score    report_type  \n",
       "0                0.8  Active Report  \n",
       "3                0.8  Active Report  \n",
       "6                0.9  Active Report  \n",
       "10               0.8  Active Report  \n",
       "19               0.7  Active Report  \n",
       "32               0.9  Active Report  \n",
       "41               0.6  Active Report  \n",
       "43               0.6  Active Report  \n",
       "44               0.9  Active Report  \n",
       "45               0.6  Active Report  \n",
       "47               0.6  Active Report  \n",
       "49               0.6  Active Report  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verified_disasters_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9c0995",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install spacy geopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc84d1ec",
   "metadata": {},
   "source": [
    "**Location Extraction** - using Named Entity Recognition extracting the Location of the post \n",
    "1. either from the user's tweet\n",
    "2. fall back to the user profile location if tweet location unavailable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46a5d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ spaCy model loaded successfully.\n",
      "‚úÖ GeoPy geocoder initialized.\n",
      "\n",
      "--- üêû Inspecting your DataFrame ---\n",
      "Columns found:\n",
      "Index(['tweet_id', 'created_at', 'text', 'author_username', 'author_verified',\n",
      "       'author_profile_location', 'tweet_geo', 'is_disaster',\n",
      "       'confidence_score', 'report_type'],\n",
      "      dtype='object')\n",
      "----------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üìç Extracting Coordinates: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:13<00:00,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Successfully found coordinates for 11 reports.\n",
      "\n",
      "--- Sample of Final Data with Coordinates ---\n",
      "                                                 text   latitude  longitude\n",
      "0   River #Ganga at #Farakka(FF) in #MURSHIDABAD d...  28.641926  77.221750\n",
      "3   River #Ganga at #CS 97 A GANGA FARAKKA in #MUR...  28.641926  77.221750\n",
      "6   ‚ÄòWorse than Amphan Cyclone‚Äô: Houses flooded, s...  22.572646  88.363895\n",
      "10  BJP is inviting sugg on how  to make Mumbai be...  19.054999  72.869203\n",
      "19  @Matt_Pinner Ofcourse yes! The same flood stor...  22.351115  78.667743\n",
      "\n",
      "üíæ Final geocoded data saved to 'final_geocoded_reports.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from geopy.geocoders import Nominatim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. Setup ---\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"‚úÖ spaCy model loaded successfully.\")\n",
    "except OSError:\n",
    "    print(\"spaCy model not found. Please run: python -m spacy download en_core_web_sm\")\n",
    "    nlp = None\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"sih_disaster_mapper_v4\")\n",
    "print(\"‚úÖ GeoPy geocoder initialized.\")\n",
    "\n",
    "# --- 2. Load Your Classified Data ---\n",
    "\n",
    "# --- DEBUGGING STEP (to confirm) ---\n",
    "print(\"\\n--- üêû Inspecting your DataFrame ---\")\n",
    "print(\"Columns found:\")\n",
    "print(verified_disasters_df.columns)\n",
    "print(\"----------------------------------\\n\")\n",
    "\n",
    "# --- 3. The Location Extraction Function (No changes needed) ---\n",
    "def get_coordinates(text, profile_location):\n",
    "    \"\"\"\n",
    "    Extracts coordinates by first checking the tweet text (NER),\n",
    "    then falling back to the user's profile location.\n",
    "    \"\"\"\n",
    "    location_name = None\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in [\"GPE\", \"LOC\"]:\n",
    "            location_name = ent.text\n",
    "            break\n",
    "            \n",
    "    if not location_name and isinstance(profile_location, str):\n",
    "        location_name = profile_location\n",
    "        \n",
    "    if location_name:\n",
    "        try:\n",
    "            location = geolocator.geocode(location_name, timeout=10)\n",
    "            if location:\n",
    "                return location.latitude, location.longitude\n",
    "        except Exception:\n",
    "            pass\n",
    "            \n",
    "    return None, None\n",
    "\n",
    "# --- 4. Apply Function and Get Final Data ---\n",
    "if nlp and not verified_disasters_df.empty:\n",
    "    tqdm.pandas(desc=\"üìç Extracting Coordinates\")\n",
    "    \n",
    "    # We directly use the 'author_profile_location' column that is already here\n",
    "    verified_disasters_df[['latitude', 'longitude']] = verified_disasters_df.progress_apply(\n",
    "        lambda row: pd.Series(get_coordinates(row['text'], row['author_profile_location'])),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Drop rows where we couldn't find any coordinates\n",
    "    final_geocoded_df = verified_disasters_df.dropna(subset=['latitude', 'longitude']).copy()\n",
    "\n",
    "    print(f\"\\n‚úÖ Successfully found coordinates for {len(final_geocoded_df)} reports.\")\n",
    "    print(\"\\n--- Sample of Final Data with Coordinates ---\")\n",
    "    print(final_geocoded_df[['text', 'latitude', 'longitude']].head())\n",
    "\n",
    "    # --- 5. Save Your Final Progress ---\n",
    "    final_geocoded_df.to_csv('final_geocoded_reports.csv', index=False)\n",
    "    print(\"\\nüíæ Final geocoded data saved to 'final_geocoded_reports.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f82cd06e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>author_username</th>\n",
       "      <th>author_verified</th>\n",
       "      <th>author_profile_location</th>\n",
       "      <th>tweet_geo</th>\n",
       "      <th>is_disaster</th>\n",
       "      <th>confidence_score</th>\n",
       "      <th>report_type</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1970351141624902010</td>\n",
       "      <td>2025-09-23 04:54:54+00:00</td>\n",
       "      <td>River #Ganga at #Farakka(FF) in #MURSHIDABAD d...</td>\n",
       "      <td>CWCOfficial_FF</td>\n",
       "      <td>False</td>\n",
       "      <td>New Delhi, India</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.8</td>\n",
       "      <td>Active Report</td>\n",
       "      <td>28.641926</td>\n",
       "      <td>77.221750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1970350823323627600</td>\n",
       "      <td>2025-09-23 04:53:38+00:00</td>\n",
       "      <td>River #Ganga at #CS 97 A GANGA FARAKKA in #MUR...</td>\n",
       "      <td>CWCOfficial_FF</td>\n",
       "      <td>False</td>\n",
       "      <td>New Delhi, India</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.8</td>\n",
       "      <td>Active Report</td>\n",
       "      <td>28.641926</td>\n",
       "      <td>77.221750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1970350658026185072</td>\n",
       "      <td>2025-09-23 04:52:58+00:00</td>\n",
       "      <td>‚ÄòWorse than Amphan Cyclone‚Äô: Houses flooded, s...</td>\n",
       "      <td>akularsnvas</td>\n",
       "      <td>False</td>\n",
       "      <td>Hyderabad, India</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Active Report</td>\n",
       "      <td>22.572646</td>\n",
       "      <td>88.363895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1970348990198849929</td>\n",
       "      <td>2025-09-23 04:46:21+00:00</td>\n",
       "      <td>BJP is inviting sugg on how  to make Mumbai be...</td>\n",
       "      <td>Sanjeevdesiguy</td>\n",
       "      <td>False</td>\n",
       "      <td>SPACE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.8</td>\n",
       "      <td>Active Report</td>\n",
       "      <td>19.054999</td>\n",
       "      <td>72.869203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1970340495512740159</td>\n",
       "      <td>2025-09-23 04:12:35+00:00</td>\n",
       "      <td>@Matt_Pinner Ofcourse yes! The same flood stor...</td>\n",
       "      <td>VedicBeat</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>Active Report</td>\n",
       "      <td>22.351115</td>\n",
       "      <td>78.667743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tweet_id                created_at  \\\n",
       "0   1970351141624902010 2025-09-23 04:54:54+00:00   \n",
       "3   1970350823323627600 2025-09-23 04:53:38+00:00   \n",
       "6   1970350658026185072 2025-09-23 04:52:58+00:00   \n",
       "10  1970348990198849929 2025-09-23 04:46:21+00:00   \n",
       "19  1970340495512740159 2025-09-23 04:12:35+00:00   \n",
       "\n",
       "                                                 text author_username  \\\n",
       "0   River #Ganga at #Farakka(FF) in #MURSHIDABAD d...  CWCOfficial_FF   \n",
       "3   River #Ganga at #CS 97 A GANGA FARAKKA in #MUR...  CWCOfficial_FF   \n",
       "6   ‚ÄòWorse than Amphan Cyclone‚Äô: Houses flooded, s...     akularsnvas   \n",
       "10  BJP is inviting sugg on how  to make Mumbai be...  Sanjeevdesiguy   \n",
       "19  @Matt_Pinner Ofcourse yes! The same flood stor...       VedicBeat   \n",
       "\n",
       "    author_verified author_profile_location  tweet_geo is_disaster  \\\n",
       "0             False        New Delhi, India        NaN        True   \n",
       "3             False        New Delhi, India        NaN        True   \n",
       "6             False        Hyderabad, India        NaN        True   \n",
       "10            False                  SPACE         NaN        True   \n",
       "19            False                    None        NaN        True   \n",
       "\n",
       "    confidence_score    report_type   latitude  longitude  \n",
       "0                0.8  Active Report  28.641926  77.221750  \n",
       "3                0.8  Active Report  28.641926  77.221750  \n",
       "6                0.9  Active Report  22.572646  88.363895  \n",
       "10               0.8  Active Report  19.054999  72.869203  \n",
       "19               0.7  Active Report  22.351115  78.667743  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verified_disasters_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d40b372",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install h3 folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2e2e0508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully loaded 11 geocoded reports.\n",
      "‚úÖ Prepared hexagon boundaries for mapping.\n",
      "\n",
      "üó∫Ô∏è  Success! Your ENHANCED hotspot map has been saved to 'disaster_hotspot_map_enhanced.html'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import h3\n",
    "import folium\n",
    "\n",
    "# --- 1. Load Your Final Geocoded Data ---\n",
    "try:\n",
    "    df = pd.read_csv('final_geocoded_reports.csv')\n",
    "    print(f\"‚úÖ Successfully loaded {len(df)} geocoded reports.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"üõë Error: 'final_geocoded_reports.csv' not found.\")\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "if not df.empty:\n",
    "    # --- 2. H3 Conversion and Density Calculation (No changes here) ---\n",
    "    H3_RESOLUTION = 7\n",
    "    df['h3_index'] = df.apply(\n",
    "        lambda row: h3.latlng_to_cell(row['latitude'], row['longitude'], H3_RESOLUTION),\n",
    "        axis=1\n",
    "    )\n",
    "    report_density = df['h3_index'].value_counts().reset_index()\n",
    "    report_density.columns = ['h3_index', 'report_count']\n",
    "\n",
    "    # --- 3. Prepare Hexagons for Visualization (No changes here) ---\n",
    "    def get_hexagon_boundary(h3_index):\n",
    "        boundary = h3.cell_to_boundary(h3_index)\n",
    "        return [[lon, lat] for lat, lon in boundary]\n",
    "    \n",
    "    report_density['hexagon'] = report_density['h3_index'].apply(get_hexagon_boundary)\n",
    "    print(\"‚úÖ Prepared hexagon boundaries for mapping.\")\n",
    "\n",
    "    # --- 4. Visualize the Hotspot Map with Enhanced Styling ---\n",
    "\n",
    "    # --- NEW: A function to determine color based on report count ---\n",
    "    def get_fill_color(count):\n",
    "        max_count = report_density['report_count'].max()\n",
    "        if count > max_count * 0.66:\n",
    "            return '#d73027' # Dark Red for high density\n",
    "        elif count > max_count * 0.33:\n",
    "            return '#fc8d59' # Orange for medium density\n",
    "        else:\n",
    "            return '#fee08b' # Yellow for low density\n",
    "\n",
    "    map_center = [df['latitude'].mean(), df['longitude'].mean()]\n",
    "    hotspot_map = folium.Map(location=map_center, zoom_start=10, tiles=\"cartodbpositron\")\n",
    "\n",
    "    for i, row in report_density.iterrows():\n",
    "        folium.GeoJson(\n",
    "            data={\n",
    "                \"type\": \"Feature\",\n",
    "                \"geometry\": {\n",
    "                    \"type\": \"Polygon\",\n",
    "                    \"coordinates\": [row['hexagon']],\n",
    "                },\n",
    "            },\n",
    "            # --- NEW: Updated style function ---\n",
    "            style_function=lambda feature, count=row['report_count']: {\n",
    "                'fillColor': get_fill_color(count), # Use our new color function\n",
    "                'color': 'white',      # A high-contrast white border\n",
    "                'weight': 2.5,         # Make the border thicker\n",
    "                'fillOpacity': 0.75,     # A consistent, strong opacity\n",
    "            },\n",
    "            tooltip=f\"<b>üî• Hotspot üî•</b><br>Reports: {row['report_count']}<br>H3 Index: {row['h3_index']}\"\n",
    "        ).add_to(hotspot_map)\n",
    "\n",
    "    hotspot_map.save(\"disaster_hotspot_map_enhanced.html\")\n",
    "    print(\"\\nüó∫Ô∏è  Success! Your ENHANCED hotspot map has been saved to 'disaster_hotspot_map_enhanced.html'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c82c7b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully loaded 11 geocoded reports.\n",
      "‚úÖ Prepared hexagon data for mapping.\n",
      "üõ∞Ô∏è  Added Satellite base layer.\n",
      "üß≠ Added clickable index and layer control.\n",
      "\n",
      "üó∫Ô∏è  Success! Your new INTERACTIVE map has been saved to 'disaster_hotspot_map_interactive.html'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import h3\n",
    "import folium\n",
    "from folium.features import MacroElement\n",
    "from jinja2 import Template\n",
    "\n",
    "# --- 1. Load Your Final Geocoded Data ---\n",
    "try:\n",
    "    df = pd.read_csv('final_geocoded_reports.csv')\n",
    "    print(f\"‚úÖ Successfully loaded {len(df)} geocoded reports.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"üõë Error: 'final_geocoded_reports.csv' not found.\")\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "if not df.empty:\n",
    "    # --- 2. H3 Conversion and Density Calculation ---\n",
    "    H3_RESOLUTION = 7\n",
    "    df['h3_index'] = df.apply(\n",
    "        lambda row: h3.latlng_to_cell(row['latitude'], row['longitude'], H3_RESOLUTION),\n",
    "        axis=1\n",
    "    )\n",
    "    report_density = df['h3_index'].value_counts().reset_index()\n",
    "    report_density.columns = ['h3_index', 'report_count']\n",
    "\n",
    "    # --- 3. Prepare Hexagons and Get Center Coordinates ---\n",
    "    def get_hexagon_data(h3_index):\n",
    "        boundary = h3.cell_to_boundary(h3_index)\n",
    "        center = h3.cell_to_latlng(h3_index)\n",
    "        return [[lon, lat] for lat, lon in boundary], center\n",
    "\n",
    "    # Apply the function to get both boundary and center\n",
    "    report_density[['hexagon', 'center']] = report_density['h3_index'].apply(\n",
    "        lambda x: pd.Series(get_hexagon_data(x))\n",
    "    )\n",
    "    print(\"‚úÖ Prepared hexagon data for mapping.\")\n",
    "\n",
    "    # --- 4. Create the Map with Satellite View ---\n",
    "    map_center = [df['latitude'].mean(), df['longitude'].mean()]\n",
    "    \n",
    "    # Initialize the map with a standard view\n",
    "    hotspot_map = folium.Map(location=map_center, zoom_start=10, tiles=\"OpenStreetMap\")\n",
    "\n",
    "    # Add the satellite tile layer\n",
    "    folium.TileLayer(\n",
    "        tiles='https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}',\n",
    "        attr='Esri',\n",
    "        name='Satellite View',\n",
    "        overlay=False,\n",
    "        control=True\n",
    "    ).add_to(hotspot_map)\n",
    "    \n",
    "    print(\"üõ∞Ô∏è  Added Satellite base layer.\")\n",
    "\n",
    "    # --- 5. Draw the Hexagonal Hotspots (no changes here) ---\n",
    "    # (Your existing code for styling and drawing hexagons)\n",
    "    def get_fill_color(count):\n",
    "        max_count = report_density['report_count'].max()\n",
    "        if count > max_count * 0.66: return '#d73027'\n",
    "        elif count > max_count * 0.33: return '#fc8d59'\n",
    "        else: return '#fee08b'\n",
    "\n",
    "    for i, row in report_density.iterrows():\n",
    "        folium.GeoJson(\n",
    "            data={\"type\": \"Feature\", \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [row['hexagon']]}},\n",
    "            style_function=lambda feature, count=row['report_count']: {\n",
    "                'fillColor': get_fill_color(count), 'color': 'white', 'weight': 2.5, 'fillOpacity': 0.75,\n",
    "            },\n",
    "            tooltip=f\"<b>üî• Hotspot üî•</b><br>Reports: {row['report_count']}<br>H3 Index: {row['h3_index']}\"\n",
    "        ).add_to(hotspot_map)\n",
    "\n",
    "    # --- 6. Create and Add the Clickable Index ---\n",
    "    # We use Jinja2 to create a custom HTML/CSS/JS element\n",
    "    template = \"\"\"\n",
    "    {% macro html(this, kwargs) %}\n",
    "    <div id=\"maplegend\" class=\"leaflet-control leaflet-bar\" \n",
    "         style=\"position: fixed; bottom: 20px; right: 20px; z-index:9999;\n",
    "                background-color: rgba(255, 255, 255, 0.8);\n",
    "                border-radius: 5px; padding: 10px; font-size:12px;\n",
    "                max-height: 200px; overflow-y: auto;\n",
    "                border: 2px solid grey;\">\n",
    "    <h4 style=\"margin-top:0;\">Hotspot Zones</h4>\n",
    "    <ul style=\"list-style-type:none; padding-left:0;\">\n",
    "    {% for i, row in this.hotspots.iterrows() %}\n",
    "        <li style=\"margin-bottom: 5px;\">\n",
    "            <a href=\"#\" style=\"text-decoration: none; color: black;\" \n",
    "               onclick=\"\n",
    "                   L.DomEvent.preventDefault(event);\n",
    "                   {{this._parent.get_name()}}.setView([{{row.center[0]}}, {{row.center[1]}}], 13);\n",
    "                   return false;\n",
    "               \">\n",
    "               Hotspot {{i+1}} ({{row.report_count}} reports)\n",
    "            </a>\n",
    "        </li>\n",
    "    {% endfor %}\n",
    "    </ul>\n",
    "    </div>\n",
    "    {% endmacro %}\n",
    "    \"\"\"\n",
    "    macro = MacroElement()\n",
    "    macro._template = Template(template)\n",
    "    # Pass the hotspot data from Python to the HTML template\n",
    "    macro.hotspots = report_density.sort_values(by='report_count', ascending=False)\n",
    "    hotspot_map.add_child(macro)\n",
    "    \n",
    "    # Add a layer control to switch between maps\n",
    "    folium.LayerControl().add_to(hotspot_map)\n",
    "    print(\"üß≠ Added clickable index and layer control.\")\n",
    "\n",
    "    # --- 7. Save the Final Map ---\n",
    "    hotspot_map.save(\"disaster_hotspot_map_interactive.html\")\n",
    "    print(\"\\nüó∫Ô∏è  Success! Your new INTERACTIVE map has been saved to 'disaster_hotspot_map_interactive.html'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dcc0528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from ddgs.ddgs import DDGS\n",
    "from PIL import Image\n",
    "\n",
    "# The main media directory\n",
    "MEDIA_DIR = \"media\"\n",
    "# The specific subdirectory for downloaded images, as per your structure\n",
    "VISUALS_DIR = os.path.join(MEDIA_DIR, \"visuals\")\n",
    "\n",
    "def get_media_visuals(keywords, location, num_images=5):\n",
    "    \"\"\"\n",
    "    Searches for and downloads relevant visuals into the 'media/visuals' directory.\n",
    "    This function is designed to be called from your core.ipynb notebook.\n",
    "    \"\"\"\n",
    "    # Create the visuals subdirectory if it doesn't exist\n",
    "    if not os.path.exists(VISUALS_DIR):\n",
    "        os.makedirs(VISUALS_DIR)\n",
    "\n",
    "    # Clean up keywords and create a search query\n",
    "    search_keywords = ' '.join(keywords) if isinstance(keywords, list) else keywords\n",
    "    search_query = f\"{search_keywords} {location}\"\n",
    "    print(f\"üé• Searching for visuals with query: '{search_query}'...\")\n",
    "    downloaded_images = []\n",
    "    \n",
    "    try:\n",
    "        with DDGS() as ddgs:\n",
    "            results = ddgs.images(\n",
    "                search_query,\n",
    "                region=\"in-en\",\n",
    "                safesearch=\"on\",\n",
    "                size=\"Large\",\n",
    "                type_image=\"photo\"\n",
    "            )\n",
    "            \n",
    "            if not results:\n",
    "                print(f\"üõë No images found for '{search_query}'.\")\n",
    "                return []\n",
    "\n",
    "            print(f\"Downloading up to {num_images} images...\")\n",
    "            # Create a unique prefix for filenames based on the location\n",
    "            location_prefix = location.split(',')[0].lower().replace(' ', '_')\n",
    "            for i, r in enumerate(results):\n",
    "                if i >= num_images:\n",
    "                    break\n",
    "                try:\n",
    "                    image_url = r.get(\"image\")\n",
    "                    if not image_url:\n",
    "                        continue\n",
    "                    \n",
    "                    image_data = requests.get(image_url, timeout=15).content\n",
    "                    # Save the file with the location prefix for easy lookup later\n",
    "                    filename = os.path.join(VISUALS_DIR, f\"{location_prefix}_image_{i}.jpg\")\n",
    "                    with open(filename, 'wb') as f:\n",
    "                        f.write(image_data)\n",
    "                    Image.open(filename).verify()\n",
    "                    downloaded_images.append(filename)\n",
    "                except Exception as e:\n",
    "                    print(f\"  - Could not download or verify an image: {e}\")\n",
    "                    \n",
    "        print(f\"‚úÖ Downloaded {len(downloaded_images)} valid images for {location}.\")\n",
    "        return downloaded_images\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"üõë Error fetching visuals: {e}\")\n",
    "        return []\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b212ea",
   "metadata": {},
   "source": [
    "# **YouTube**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc2684e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade google-api-python-client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43fb49c",
   "metadata": {},
   "source": [
    "# **Bending the space-time to mind bogggling innovation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2083462",
   "metadata": {},
   "source": [
    "**Media Persona**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4857375",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=\"openai/gpt-oss-120b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7d1dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade selenium webdriver-manager groq python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7af726a",
   "metadata": {},
   "source": [
    "**SetUP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "008735e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Media and asset directories are set.\n",
      "‚úÖ Groq client initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.edge.service import Service as EdgeService\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "\n",
    "# --- DIRECTORY & FILE SETUP ---\n",
    "ASSETS_DIR = \"assets\"\n",
    "MEDIA_DIR = \"media\"\n",
    "if not os.path.exists(MEDIA_DIR):\n",
    "    os.makedirs(MEDIA_DIR)\n",
    "print(f\"‚úÖ Media and asset directories are set.\")\n",
    "\n",
    "# --- CRITICAL PATHS & FILENAMES ---\n",
    "# These are the assets you have already created and stored in the 'assets' folder\n",
    "TEMPLATE_VIDEO_PATH = os.path.abspath(os.path.join(ASSETS_DIR, \"0_News_Globe_3840x2160.mp4\"))\n",
    "\n",
    "# --- CREDENTIALS & CONFIGURATION ---\n",
    "load_dotenv()\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "HEYGEN_EMAIL = os.getenv(\"HEYGEN_EMAIL\")\n",
    "HEYGEN_PASSWORD = os.getenv(\"HEYGEN_PASSWORD\")\n",
    "\n",
    "if not all([GROQ_API_KEY, HEYGEN_EMAIL, HEYGEN_PASSWORD]):\n",
    "    print(\"üõë ERROR: One or more credentials (Groq, HeyGen Email/Password) are missing. Check your .env file.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# This is the detailed voice direction guide you created\n",
    "VOICE_DIRECTION_GUIDE = \"\"\"\n",
    "Voice: Sharp, tense, urgent and forceful, projecting anger and frustration clearly through intensity and firmness.\n",
    "Tone: Harsh, impatient, and strained, conveying a strong sense of irritation and exasperation.\n",
    "Punctuation: Short, abrupt sentences punctuated with pronounced pauses, reflecting agitation and urgency.\n",
    "Delivery: Quickened and intense, emphasizing key words and phrases strongly to express heightened emotional stress.\n",
    "Phrasing: Direct and accusatory, with rising and falling intonation to underscore irritation and dissatisfaction.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize Groq client\n",
    "groq_client = Groq(api_key=GROQ_API_KEY)\n",
    "print(\"‚úÖ Groq client initialized successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bee2bd5",
   "metadata": {},
   "source": [
    "**Script Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "adb32bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hindi_news_script(location, summary):\n",
    "    \"\"\"Generates a professional Hindi news script using the Groq API.\"\"\"\n",
    "    print(\"\\nü§ñ Generating Hindi news script with Groq Llama3-70B...\")\n",
    "    prompt_messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are Sharmistha (‡§∂‡§∞‡•ç‡§Æ‡§ø‡§∑‡•ç‡§†‡§æ), a top news anchor for 'UthoIndia'. Your tone is sharp, urgent, and intense. Create a concise, powerful Hindi news report (~200 characters) following an Intro-Headline-Details structure. Your entire output must ONLY be the raw Hindi text of the script itself.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"**Disaster Data:**\\n- Location: \\\"{location}\\\"\\n- Summary: \\\"{summary}\\\"\"}\n",
    "    ]\n",
    "    try:\n",
    "        chat_completion = groq_client.chat.completions.create(messages=prompt_messages, model=model)\n",
    "        script_text = chat_completion.choices[0].message.content\n",
    "        print(\"‚úÖ Script generated successfully.\")\n",
    "        return script_text\n",
    "    except Exception as e:\n",
    "        print(f\"üõë Error generating script: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1eadc5",
   "metadata": {},
   "source": [
    "**Agent Vid Gen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6a33ecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_video_with_copilot(email, password, script_text, voice_direction, background_video_path):\n",
    "    \"\"\"\n",
    "    Automates the HeyGen website using Selenium to generate the final news broadcast.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- ü§ñ UthoIndia Copilot is taking control of Microsoft Edge ---\")\n",
    "    \n",
    "    options = webdriver.EdgeOptions()\n",
    "    prefs = {\"download.default_directory\": os.path.abspath(MEDIA_DIR)}\n",
    "    options.add_experimental_option(\"prefs\", prefs)\n",
    "    \n",
    "    # Automatically install and manage the Edge driver\n",
    "    driver = webdriver.Edge(service=EdgeService(EdgeChromiumDriverManager().install()), options=options)\n",
    "    wait = WebDriverWait(driver, 120) # Wait up to 120 seconds for elements to appear\n",
    "    \n",
    "    try:\n",
    "        # --- Step 1: Login ---\n",
    "        print(\"   - Step 1/6: Logging into HeyGen...\")\n",
    "        driver.get(\"https://app.heygen.com/login\")\n",
    "        wait.until(EC.presence_of_element_located((By.NAME, \"email\"))).send_keys(email)\n",
    "        wait.until(EC.presence_of_element_located((By.NAME, \"password\"))).send_keys(password)\n",
    "        wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[contains(., 'Sign in')]\"))).click()\n",
    "        print(\"     ...Login successful.\")\n",
    "\n",
    "        # --- Step 2: Navigate to Video Creation ---\n",
    "        print(\"   - Step 2/6: Navigating to video creation...\")\n",
    "        wait.until(EC.url_contains(\"home\")) # Wait for the main dashboard to load\n",
    "        wait.until(EC.element_to_be_clickable((By.XPATH, \"//*[contains(text(), 'Create Video')]\"))).click()\n",
    "        wait.until(EC.element_to_be_clickable((By.XPATH, \"//*[contains(text(), 'Landscape')]\"))).click()\n",
    "        print(\"     ...Ready to create in landscape mode.\")\n",
    "\n",
    "        # --- Step 3: Upload Template & Select Avatar ---\n",
    "        print(\"   - Step 3/6: Uploading assets and selecting avatar...\")\n",
    "        \n",
    "        # This XPath finds a file input that might be hidden. It's a robust method.\n",
    "        background_uploader = wait.until(EC.presence_of_element_located((By.XPATH, \"//input[@type='file']\")))\n",
    "        background_uploader.send_keys(background_video_path)\n",
    "        \n",
    "        # Select Sharmistha Avatar (assuming her name is in the alt text of the image)\n",
    "        wait.until(EC.element_to_be_clickable((By.XPATH, \"//img[@alt='Sharmistha Reporter']\"))).click()\n",
    "        print(\"     ...Assets configured.\")\n",
    "        \n",
    "        # --- Step 4: Add Script, Voice, and Direction ---\n",
    "        print(\"   - Step 4/6: Setting script and voice options...\")\n",
    "        \n",
    "        # Paste the script\n",
    "        script_textarea = wait.until(EC.presence_of_element_located((By.XPATH, \"//textarea[contains(@placeholder, 'script')]\")))\n",
    "        script_textarea.clear()\n",
    "        script_textarea.send_keys(script_text)\n",
    "        \n",
    "        # Select Voice (e.g., Anika Mehra)\n",
    "        wait.until(EC.element_to_be_clickable((By.XPATH, \"//*[contains(text(), 'Anika')]\"))).click() # A simplified selector\n",
    "        \n",
    "        # Open and paste Voice Direction\n",
    "        wait.until(EC.element_to_be_clickable((By.XPATH, \"//*[contains(text(), 'Voice Direction')]\"))).click()\n",
    "        direction_textarea = wait.until(EC.presence_of_element_located((By.XPATH, \"//textarea[contains(@placeholder, 'voice direction')]\")))\n",
    "        direction_textarea.send_keys(voice_direction)\n",
    "        print(\"     ...Script and voice direction set.\")\n",
    "\n",
    "        # --- Step 5: Set Motion Engine ---\n",
    "        print(\"   - Step 5/6: Enabling Unlimited Motion Engine...\")\n",
    "        wait.until(EC.element_to_be_clickable((By.XPATH, \"//label[contains(., 'Unlimited Motion')]\"))).click()\n",
    "        print(\"     ...Motion engine enabled.\")\n",
    "\n",
    "        # --- Step 6: Generate and Download ---\n",
    "        print(\"   - Step 6/6: Submitting job and waiting for download...\")\n",
    "        wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[contains(., 'Submit')]\"))).click()\n",
    "        \n",
    "        # Wait for the video to be processed and the download button to appear on the next page\n",
    "        wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[contains(., 'Download')]\"))).click()\n",
    "        \n",
    "        # Wait for the download to complete\n",
    "        print(\"     ...Download initiated. Waiting for 120 seconds for the file to save.\")\n",
    "        time.sleep(120)\n",
    "\n",
    "        print(f\"\\nüî•üî•üî• IT'S DONE! The Copilot has downloaded the final broadcast to your '{MEDIA_DIR}' folder! üî•üî•üî•\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"üõë Error during browser automation: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        print(\"   - Closing the browser.\")\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addd83d7",
   "metadata": {},
   "source": [
    "**AI Report Showdown**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "485fea47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Generating Hindi news script with Groq Llama3-70B...\n",
      "‚úÖ Script generated successfully.\n",
      "\n",
      "--- ü§ñ UthoIndia Copilot is taking control of Microsoft Edge ---\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "Could not reach host. Are you offline?",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mgaierror\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DEEPMALYA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\urllib3\\connection.py:198\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     sock = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m socket.gaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36m_patched_create_connection\u001b[39m\u001b[34m(address, *args, **kwargs)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# For any other website (like DuckDuckGo), it works normally.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_original_create_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DEEPMALYA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\urllib3\\util\\connection.py:60\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, label empty or too long\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     61\u001b[39m     af, socktype, proto, canonname, sa = res\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DEEPMALYA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\socket.py:977\u001b[39m, in \u001b[36mgetaddrinfo\u001b[39m\u001b[34m(host, port, family, type, proto, flags)\u001b[39m\n\u001b[32m    976\u001b[39m addrlist = []\n\u001b[32m--> \u001b[39m\u001b[32m977\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    978\u001b[39m     af, socktype, proto, canonname, sa = res\n",
      "\u001b[31mgaierror\u001b[39m: [Errno 11001] getaddrinfo failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mNameResolutionError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DEEPMALYA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DEEPMALYA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\urllib3\\connectionpool.py:488\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    487\u001b[39m         new_e = _wrap_proxy_error(new_e, conn.proxy.scheme)\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[32m    490\u001b[39m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[32m    491\u001b[39m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DEEPMALYA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\urllib3\\connectionpool.py:464\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DEEPMALYA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\urllib3\\connectionpool.py:1093\u001b[39m, in \u001b[36mHTTPSConnectionPool._validate_conn\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conn.is_closed:\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DEEPMALYA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\urllib3\\connection.py:753\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    752\u001b[39m sock: socket.socket | ssl.SSLSocket\n\u001b[32m--> \u001b[39m\u001b[32m753\u001b[39m \u001b[38;5;28mself\u001b[39m.sock = sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    754\u001b[39m server_hostname: \u001b[38;5;28mstr\u001b[39m = \u001b[38;5;28mself\u001b[39m.host\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DEEPMALYA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\urllib3\\connection.py:205\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m socket.gaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m.host, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mNameResolutionError\u001b[39m: <urllib3.connection.HTTPSConnection object at 0x000001CE162B8550>: Failed to resolve 'msedgedriver.azureedge.net' ([Errno 11001] getaddrinfo failed)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mMaxRetryError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DEEPMALYA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\requests\\adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DEEPMALYA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\urllib3\\connectionpool.py:841\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    839\u001b[39m     new_e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, new_e)\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m retries.sleep()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DEEPMALYA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\urllib3\\util\\retry.py:519\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    518\u001b[39m     reason = error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    521\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mIncremented Retry for (url=\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m, url, new_retry)\n",
      "\u001b[31mMaxRetryError\u001b[39m: HTTPSConnectionPool(host='msedgedriver.azureedge.net', port=443): Max retries exceeded with url: /LATEST_RELEASE_140_WINDOWS (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001CE162B8550>: Failed to resolve 'msedgedriver.azureedge.net' ([Errno 11001] getaddrinfo failed)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DEEPMALYA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\webdriver_manager\\core\\http.py:32\u001b[39m, in \u001b[36mWDMHttpClient.get\u001b[39m\u001b[34m(self, url, **kwargs)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     resp = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ssl_verify\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.ConnectionError:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DEEPMALYA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\requests\\api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m:rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DEEPMALYA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DEEPMALYA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DEEPMALYA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DEEPMALYA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\requests\\adapters.py:677\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    675\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request=request)\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request=request)\n\u001b[32m    679\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mConnectionError\u001b[39m: HTTPSConnectionPool(host='msedgedriver.azureedge.net', port=443): Max retries exceeded with url: /LATEST_RELEASE_140_WINDOWS (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001CE162B8550>: Failed to resolve 'msedgedriver.azureedge.net' ([Errno 11001] getaddrinfo failed)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[86]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      7\u001b[39m hindi_script = generate_hindi_news_script(hotspot_location, hotspot_summary)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hindi_script:\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# This is the final call to the AI Copilot\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[43mgenerate_video_with_copilot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43memail\u001b[49m\u001b[43m=\u001b[49m\u001b[43mHEYGEN_EMAIL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[43mHEYGEN_PASSWORD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscript_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhindi_script\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvoice_direction\u001b[49m\u001b[43m=\u001b[49m\u001b[43mVOICE_DIRECTION_GUIDE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackground_video_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTEMPLATE_VIDEO_PATH\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     19\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müõë Pipeline stopped: Could not generate a script.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[85]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mgenerate_video_with_copilot\u001b[39m\u001b[34m(email, password, script_text, voice_direction, background_video_path)\u001b[39m\n\u001b[32m      9\u001b[39m options.add_experimental_option(\u001b[33m\"\u001b[39m\u001b[33mprefs\u001b[39m\u001b[33m\"\u001b[39m, prefs)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Automatically install and manage the Edge driver\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m driver = webdriver.Edge(service=EdgeService(\u001b[43mEdgeChromiumDriverManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43minstall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m), options=options)\n\u001b[32m     13\u001b[39m wait = WebDriverWait(driver, \u001b[32m120\u001b[39m) \u001b[38;5;66;03m# Wait up to 120 seconds for elements to appear\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# --- Step 1: Login ---\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DEEPMALYA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\webdriver_manager\\microsoft.py:73\u001b[39m, in \u001b[36mEdgeChromiumDriverManager.install\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minstall\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     driver_path = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_driver_binary_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m     os.chmod(driver_path, \u001b[32m0o755\u001b[39m)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m driver_path\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DEEPMALYA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\webdriver_manager\\core\\manager.py:35\u001b[39m, in \u001b[36mDriverManager._get_driver_binary_path\u001b[39m\u001b[34m(self, driver)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_driver_binary_path\u001b[39m(\u001b[38;5;28mself\u001b[39m, driver):\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     binary_path = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cache_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind_driver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m binary_path:\n\u001b[32m     37\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m binary_path\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DEEPMALYA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\webdriver_manager\\core\\driver_cache.py:107\u001b[39m, in \u001b[36mDriverCacheManager.find_driver\u001b[39m\u001b[34m(self, driver)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m browser_version:\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m driver_version = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_cache_key_driver_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m metadata = \u001b[38;5;28mself\u001b[39m.load_metadata_content()\n\u001b[32m    110\u001b[39m key = \u001b[38;5;28mself\u001b[39m.__get_metadata_key(driver)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DEEPMALYA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\webdriver_manager\\core\\driver_cache.py:154\u001b[39m, in \u001b[36mDriverCacheManager.get_cache_key_driver_version\u001b[39m\u001b[34m(self, driver)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._cache_key_driver_version:\n\u001b[32m    153\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._cache_key_driver_version\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdriver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_driver_version_to_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DEEPMALYA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\webdriver_manager\\core\\driver.py:48\u001b[39m, in \u001b[36mDriver.get_driver_version_to_download\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._driver_version_to_download:\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._driver_version_to_download\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_latest_release_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DEEPMALYA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\webdriver_manager\\drivers\\edge.py:51\u001b[39m, in \u001b[36mEdgeChromiumDriver.get_latest_release_version\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     42\u001b[39m os_type = \u001b[38;5;28mself\u001b[39m._os_system_manager.get_os_type()\n\u001b[32m     43\u001b[39m latest_release_url = {\n\u001b[32m     44\u001b[39m     OSType.WIN\n\u001b[32m     45\u001b[39m     \u001b[38;5;129;01min\u001b[39;00m os_type: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._latest_release_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmajor_edge_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_WINDOWS\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     49\u001b[39m     \u001b[38;5;129;01min\u001b[39;00m os_type: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._latest_release_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmajor_edge_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_LINUX\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     50\u001b[39m }[\u001b[38;5;28;01mTrue\u001b[39;00m]\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_http_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlatest_release_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp.text.rstrip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DEEPMALYA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\webdriver_manager\\core\\http.py:35\u001b[39m, in \u001b[36mWDMHttpClient.get\u001b[39m\u001b[34m(self, url, **kwargs)\u001b[39m\n\u001b[32m     32\u001b[39m     resp = requests.get(\n\u001b[32m     33\u001b[39m         url=url, verify=\u001b[38;5;28mself\u001b[39m._ssl_verify, stream=\u001b[38;5;28;01mTrue\u001b[39;00m, **kwargs)\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.ConnectionError:\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.ConnectionError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not reach host. Are you offline?\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     36\u001b[39m \u001b[38;5;28mself\u001b[39m.validate_response(resp)\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[31mConnectionError\u001b[39m: Could not reach host. Are you offline?"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # --- Live Hotspot Data for Kolkata ---\n",
    "    hotspot_location = \"Kolkata, West Bengal\"\n",
    "    hotspot_summary = f\"As of {time.strftime('%I:%M %p IST')}, heavy and untimely overnight rainfall has led to severe street flooding.\"\n",
    "\n",
    "    # --- Run the Agent's Workflow ---\n",
    "    hindi_script = generate_hindi_news_script(hotspot_location, hotspot_summary)\n",
    "    \n",
    "    if hindi_script:\n",
    "        # This is the final call to the AI Copilot\n",
    "        generate_video_with_copilot(\n",
    "            email=HEYGEN_EMAIL,\n",
    "            password=HEYGEN_PASSWORD,\n",
    "            script_text=hindi_script,\n",
    "            voice_direction=VOICE_DIRECTION_GUIDE,\n",
    "            background_video_path=TEMPLATE_VIDEO_PATH\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\nüõë Pipeline stopped: Could not generate a script.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4892308",
   "metadata": {},
   "source": [
    "# Rand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45b9bbc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
