{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2749270c",
   "metadata": {},
   "source": [
    "# **Data Ingestion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ac9f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65377b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Authentication successful!\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary library\n",
    "import tweepy\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Client object to interact with the API\n",
    "client = tweepy.Client(os.getenv(\"bearer_token\"))\n",
    "\n",
    "print(\"‚úÖ Authentication successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c10c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gempa 5,2 M Guncang Simeulue, Tidak Berpotensi Tsunami, BMKG Imbau Warga Tetap Tenang\n",
      "\n",
      "Gempa dengan kekuatan Magnitudo 5,2 kembali mengguncang wilayah kepulauan Aceh tepatnya di Kota Sinabang, Kabupaten Simeulue, Selasa (23/9/2025).\n",
      "\n",
      "#Gempa #GempaDiAceh\n",
      "\n",
      "https://t.co/uHuKzhscn4\n",
      "---\n",
      "Setiap sudut Museum Tsunami Aceh menyimpan jejak luka dan harapan.\n",
      "Bukan sekadar bangunan, tapi simbol kekuatan dan ingatan.\n",
      "Arsitekturnya berbicara tentang kehilangan, perjuangan, dan bangkit kembali. #museumtsunami https://t.co/SPYJbSO9aH\n",
      "---\n",
      "@tsunami_lonely YOONGI üò≠ He really posted üíú\n",
      "---\n",
      "walaupun melewati badai ombak tsunami dulu ges gpp aku bercaya üòî\n",
      "https://t.co/egq5jVgrUL\n",
      "---\n",
      "Guten Morgen! ‚òïÔ∏èüíô\n",
      "\n",
      "So darf ein Dienstag gerne starten. ‚òùÔ∏èüòé\n",
      "\n",
      "Der Wind dreht sich! ‚ö†Ô∏è Der blaue Tsunami üåä trifft langsam aber sicher das Land‚Ä¶ Wir sind n√§her am Ziel, als wir denken. üòÆ‚Äçüí®üî• https://t.co/cHdCpmBEn8\n",
      "---\n",
      "Kjjj ganamos con media reserva, Funes Mori, y Alario hizo medio gol, ma√±ana lo m√°s tranquilo es un tsunami.\n",
      "---\n",
      "@Alphafox78 Well, getting into my tsunami pod bunker. Hold my breath and then pop out on the other side of the flood. Easy Peasy\n",
      "---\n",
      "@humoorrer @vasilyaarkhipov @Theunk13 IAF ops achievements: Air superiority in 1965/1971 Indo-Pak wars (e.g., 1971 enabled Bangladesh creation); Kargil strikes (1999); Balakot airstrikes (2019); UN Congo peacekeeping (1960s); Siachen airlifts (1984); disaster relief like 2004 tsunami.\n",
      "\n",
      "PLAAF ops achievements: Korean\n",
      "---\n",
      "@algi_zaki61369 @bankrbot Jacking into the matrix to ride OVPP's digital tsunami straight to the moon.\n",
      "\n",
      "OVPP deployed to: 0x4A829Dc75422c316c7Ef4aBa9f608A079C85C170\n",
      "\n",
      "View on: https://t.co/8wT1HO8dum\n",
      "---\n",
      "@brunettecannons \"After I finish tasting, then I want to start deep plumbing and digging  and laying down pipe into your abyss while touching your Cervical womb driving it mad insane while you beg me to send my Tsunami all in you\"!!!!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Define a simple search query\n",
    "# The '-is:retweet' part is useful to avoid duplicates and get original posts\n",
    "query = \"tsunami -is:retweet\"\n",
    "\n",
    "# Use the client to search for recent tweets that match the query. We'll just ask for 10 tweets to start with\n",
    "response = client.search_recent_tweets(query=query, max_results=10)\n",
    "\n",
    "# The data is inside the 'data' attribute of the response\n",
    "tweets = response.data\n",
    "\n",
    "# Let's check if we got anything and print the text of each tweet\n",
    "if tweets:\n",
    "    for tweet in tweets:\n",
    "        print(tweet.text)\n",
    "        print(\"---\")\n",
    "else:\n",
    "    print(\"No tweets found for this query.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea2bd7f",
   "metadata": {},
   "source": [
    "### **Raising the Bar - A More Powerful Search üìà**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5360206e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching with query: (tsunami OR flood OR cyclone OR highwave OR stormsurge OR highwaves OR coastalflood) (India OR Chennai OR Mumbai OR Kerala OR Odisha OR Bengal) -is:retweet lang:en\n"
     ]
    }
   ],
   "source": [
    "# 1. Build a more complex query\n",
    "# Use OR to look for any of these words. Use () to group them.\n",
    "# 'lang:en' filters for English language tweets.\n",
    "keywords = \"(tsunami OR flood OR cyclone OR highwave OR stormsurge OR highwaves OR coastalflood)\"\n",
    "language = \"lang:en\"\n",
    "# Geo-filtering is very limited in the standard API. A good trick is to add place names.\n",
    "# This isn't perfect, but it's a great start.\n",
    "places = \"(India OR Chennai OR Mumbai OR Kerala OR Odisha OR Bengal)\"\n",
    "\n",
    "# Combine everything into one query\n",
    "# The query now looks for any of our keywords AND any of our places.\n",
    "final_query = f\"{keywords} {places} -is:retweet {language}\"\n",
    "\n",
    "print(f\"Searching with query: {final_query}\")\n",
    "\n",
    "# 2. Ask for more data fields. We want to know who posted it, when, and if there's any location info.\n",
    "response = client.search_recent_tweets(\n",
    "    query=final_query,\n",
    "    max_results=50,  # Let's get 100 tweets this time\n",
    "    tweet_fields=[\"created_at\", \"author_id\", \"geo\"], # What info we want about the tweet\n",
    "    user_fields=[\"username\", \"verified\", \"location\"], # What info we want about the user\n",
    "    expansions=[\"author_id\", \"geo.place_id\"] # Tells the API to give us the full user and place objects\n",
    ")\n",
    "\n",
    "# The data is more complex now, so we need a good way to store it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdb2effa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Success! Saved 50 tweets to my_collected_tweets.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# This assumes the 'response' variable from your last run is still in memory\n",
    "tweet_data = []\n",
    "\n",
    "# The response includes the main tweet data and a separate 'includes' dictionary\n",
    "if response.data:\n",
    "    users = {user[\"id\"]: user for user in response.includes['users']}\n",
    "    for tweet in response.data:\n",
    "        author = users[tweet.author_id]\n",
    "        \n",
    "        # Create a clean dictionary for each tweet\n",
    "        tweet_dict = {\n",
    "            \"tweet_id\": tweet.id,\n",
    "            \"created_at\": str(tweet.created_at), # Convert to string for JSON\n",
    "            \"text\": tweet.text,\n",
    "            \"author_username\": author.username,\n",
    "            \"author_verified\": author.verified,\n",
    "            \"author_profile_location\": author.location,\n",
    "            \"tweet_geo\": tweet.geo \n",
    "        }\n",
    "        tweet_data.append(tweet_dict)\n",
    "\n",
    "# Now, save the collected data to a file\n",
    "file_name = \"my_collected_tweets.json\"\n",
    "with open(file_name, \"w\") as f:\n",
    "    json.dump(tweet_data, f, indent=4)\n",
    "\n",
    "print(f\"‚úÖ Success! Saved {len(tweet_data)} tweets to {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4205be",
   "metadata": {},
   "source": [
    "**Model Client**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "503b1232",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DEEPMALYA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Gemini model initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- 1. Setup ---\n",
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# Initialize the Gemini 1.5 Flash model\n",
    "# NOTE: The correct model name is 'gemini-1.5-flash-latest'\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "print(\"‚úÖ Gemini model initialized successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef38d45",
   "metadata": {},
   "source": [
    "**Classifier Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc2295b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. The New, More Sophisticated Batch Classifier ---\n",
    "def classify_tweet_batch_v2(tweet_batch):\n",
    "    \"\"\"\n",
    "    Uses the Gemini LLM with highly specific instructions to identify ACTIVE disaster reports.\n",
    "    \"\"\"\n",
    "    tweets_for_prompt = \"\\n\".join([f'{i+1}. \"{text}\"' for i, text in enumerate(tweet_batch)])\n",
    "\n",
    "    # --- THE REFINED PROMPT ---\n",
    "    prompt = f\"\"\"\n",
    "        You are a real-time situational awareness analyst. Your task is to identify ACTIVE, first-hand reports of ongoing natural disasters or emergencies from a list of texts.\n",
    "\n",
    "        **CRITICAL Instructions:**\n",
    "        1.  Your goal is to find reports from people on the ground, not news or discussions.\n",
    "        2.  A text is only a \"disaster\" if it describes an event happening NOW or very recently.\n",
    "        3.  **EXCLUDE** the following:\n",
    "            - News articles reporting on past or future events.\n",
    "            - General discussions about disaster preparedness.\n",
    "            - Political promises or policy suggestions (e.g., \"we need better cyclone protection\").\n",
    "            - Figurative language or metaphors (e.g., \"a tsunami of fans\").\n",
    "        4.  Provide your response ONLY in a valid JSON array format. Each object must have four keys: \"id\" (the input number), \"is_disaster\" (boolean), \"confidence_score\" (float reflecting severity), and \"report_type\" (string: \"Active Report\", \"News/Discussion\", or \"Figurative/Unrelated\").\n",
    "\n",
    "        **List of Texts to Analyze:**\n",
    "        {tweets_for_prompt}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        time.sleep(5)\n",
    "        response = model.generate_content(prompt)\n",
    "        json_response = json.loads(response.text.strip().replace(\"```json\", \"\").replace(\"```\", \"\"))\n",
    "        return json_response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during batch classification: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef793589",
   "metadata": {},
   "source": [
    "**Batch_Classifier function** - Instead of sending 20 requests to the LLM for 20 times and hitting the limit we send 20 requests as a whole batch one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "742088f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Function to Create Batches ---\n",
    "def create_batches(data, batch_size=20):\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield data[i:i + batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac953b65",
   "metadata": {},
   "source": [
    "**Dummy Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980c2cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Test Cases ---\n",
      "Text: 'A Tsunami of fans flooded the road when MS. Dhoni came out'\n",
      "Is Disaster: False (Score: 0.00)\n",
      "\n",
      "Text: 'Alert: Water logging reported in several parts of Mumbai after heavy rain.'\n",
      "Is Disaster: True (Score: 0.70)\n",
      "\n",
      "Text: 'Just enjoying a beautiful sunset at the beach today!'\n",
      "Is Disaster: False (Score: 0.00)\n",
      "\n",
      "Text: 'BREAKING: Cyclone is expected to make landfall on the east coast tomorrow morning with winds over 150 km/h.'\n",
      "Is Disaster: True (Score: 0.95)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 3. The New Test Function ---\n",
    "def run_test_cases():\n",
    "    print(\"\\n--- Running Test Cases ---\")\n",
    "    \n",
    "    # Create a list of test cases to send as one batch\n",
    "    test_tweets = [\n",
    "        \"A Tsunami of fans flooded the road when MS. Dhoni came out\", # Metaphor\n",
    "        \"Alert: Water logging reported in several parts of Mumbai after heavy rain.\", # Real, low severity\n",
    "        \"Just enjoying a beautiful sunset at the beach today!\", # Not a disaster\n",
    "        \"BREAKING: Cyclone is expected to make landfall on the east coast tomorrow morning with winds over 150 km/h.\" # Real, high severity\n",
    "    ]\n",
    "\n",
    "    # Call the batch function once with the entire list\n",
    "    results = classify_tweet_batch_v2(test_tweets)\n",
    "\n",
    "    if not results:\n",
    "        print(\"Could not get results from the API.\")\n",
    "        return\n",
    "\n",
    "    # Neatly print the result for each test case\n",
    "    for result in results:\n",
    "        # The 'id' from the result corresponds to the position in our test_tweets list\n",
    "        # We subtract 1 because lists are 0-indexed\n",
    "        tweet_index = result['id'] - 1\n",
    "        tweet_text = test_tweets[tweet_index]\n",
    "        is_dis = result['is_disaster']\n",
    "        score = result['confidence_score']\n",
    "        \n",
    "        print(f\"Text: '{tweet_text}'\")\n",
    "        print(f\"Is Disaster: {is_dis} (Score: {score:.2f})\\n\")\n",
    "\n",
    "# --- Run the Test ---\n",
    "run_test_cases()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741807e1",
   "metadata": {},
   "source": [
    "**Augmenting with data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "476fbc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully loaded 50 tweets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches with v2 Prompt:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches with v2 Prompt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:34<00:00, 11.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ‚úÖ Classification Complete! ---\n",
      "Found 12 high-confidence ACTIVE disaster reports.\n",
      "\n",
      "--- Sample of Verified Reports ---\n",
      "                                                 text  confidence_score\n",
      "6   ‚ÄòWorse than Amphan Cyclone‚Äô: Houses flooded, s...               0.9\n",
      "32  @motherrr @manjusipayya @dksardana @Ambarseriy...               0.9\n",
      "44  ‚ÄòWorse than Amphan Cyclone‚Äô: Houses flooded, s...               0.9\n",
      "0   River #Ganga at #Farakka(FF) in #MURSHIDABAD d...               0.8\n",
      "10  BJP is inviting sugg on how  to make Mumbai be...               0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Load Data and Process ---\n",
    "try:\n",
    "    df = pd.read_json('my_collected_tweets.json')\n",
    "    print(f\"\\nSuccessfully loaded {len(df)} tweets.\")\n",
    "    \n",
    "    all_results = []\n",
    "    tweet_list = df['text'].tolist()\n",
    "    \n",
    "    for batch in tqdm(list(create_batches(tweet_list, 20)), desc=\"Processing Batches with v2 Prompt\"):\n",
    "        batch_results = classify_tweet_batch_v2(batch)\n",
    "        all_results.extend(batch_results)\n",
    "\n",
    "    # --- 5. Map Results and Filter ---\n",
    "    if all_results:\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        results_df['index'] = results_df['id'] - 1\n",
    "        df = df.reset_index().merge(results_df, on='index', how='left').drop(columns=['index', 'id'])\n",
    "\n",
    "        # --- THE NEW FILTERING LOGIC ---\n",
    "        # We now filter for \"Active Report\" type for much higher accuracy\n",
    "        verified_disasters_df = df[\n",
    "            (df['is_disaster'] == True) & \n",
    "            (df['report_type'] == 'Active Report')\n",
    "        ].copy()\n",
    "\n",
    "        print(\"\\n--- ‚úÖ Classification Complete! ---\")\n",
    "        print(f\"Found {len(verified_disasters_df)} high-confidence ACTIVE disaster reports.\")\n",
    "        print(\"\\n--- Sample of Verified Reports ---\")\n",
    "        print(verified_disasters_df.sort_values(by='confidence_score', ascending=False)[['text', 'confidence_score']].head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"\\nError: 'my_collected_tweets.json' not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24617baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>confidence_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>‚ÄòWorse than Amphan Cyclone‚Äô: Houses flooded, s...</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>@motherrr @manjusipayya @dksardana @Ambarseriy...</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>‚ÄòWorse than Amphan Cyclone‚Äô: Houses flooded, s...</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>River #Ganga at #Farakka(FF) in #MURSHIDABAD d...</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>BJP is inviting sugg on how  to make Mumbai be...</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>River #Ganga at #CS 97 A GANGA FARAKKA in #MUR...</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>@Matt_Pinner Ofcourse yes! The same flood stor...</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>@RealDrJaneRuby Literally the most synchronize...</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>@BaddieDean @hoar27796 @Codex_India3 thanx to ...</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>‚ÄòWorse than Amphan Cyclone‚Äô: Houses flooded, s...</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>@influenya @CelticAshes As a Black person, thi...</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Cyclone bulletin and #Navratri\\nWeather update...</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  confidence_score\n",
       "6   ‚ÄòWorse than Amphan Cyclone‚Äô: Houses flooded, s...               0.9\n",
       "32  @motherrr @manjusipayya @dksardana @Ambarseriy...               0.9\n",
       "44  ‚ÄòWorse than Amphan Cyclone‚Äô: Houses flooded, s...               0.9\n",
       "0   River #Ganga at #Farakka(FF) in #MURSHIDABAD d...               0.8\n",
       "10  BJP is inviting sugg on how  to make Mumbai be...               0.8\n",
       "3   River #Ganga at #CS 97 A GANGA FARAKKA in #MUR...               0.8\n",
       "19  @Matt_Pinner Ofcourse yes! The same flood stor...               0.7\n",
       "41  @RealDrJaneRuby Literally the most synchronize...               0.6\n",
       "43  @BaddieDean @hoar27796 @Codex_India3 thanx to ...               0.6\n",
       "45  ‚ÄòWorse than Amphan Cyclone‚Äô: Houses flooded, s...               0.6\n",
       "47  @influenya @CelticAshes As a Black person, thi...               0.6\n",
       "49  Cyclone bulletin and #Navratri\\nWeather update...               0.6"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verified_disasters_df.sort_values(by='confidence_score', ascending=False)[['text', 'confidence_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0927e6dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 10)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verified_disasters_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca5326ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>author_username</th>\n",
       "      <th>author_verified</th>\n",
       "      <th>author_profile_location</th>\n",
       "      <th>tweet_geo</th>\n",
       "      <th>is_disaster</th>\n",
       "      <th>confidence_score</th>\n",
       "      <th>report_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1970351141624902010</td>\n",
       "      <td>2025-09-23 04:54:54+00:00</td>\n",
       "      <td>River #Ganga at #Farakka(FF) in #MURSHIDABAD d...</td>\n",
       "      <td>CWCOfficial_FF</td>\n",
       "      <td>False</td>\n",
       "      <td>New Delhi, India</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.8</td>\n",
       "      <td>Active Report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1970350823323627600</td>\n",
       "      <td>2025-09-23 04:53:38+00:00</td>\n",
       "      <td>River #Ganga at #CS 97 A GANGA FARAKKA in #MUR...</td>\n",
       "      <td>CWCOfficial_FF</td>\n",
       "      <td>False</td>\n",
       "      <td>New Delhi, India</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.8</td>\n",
       "      <td>Active Report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1970350658026185072</td>\n",
       "      <td>2025-09-23 04:52:58+00:00</td>\n",
       "      <td>‚ÄòWorse than Amphan Cyclone‚Äô: Houses flooded, s...</td>\n",
       "      <td>akularsnvas</td>\n",
       "      <td>False</td>\n",
       "      <td>Hyderabad, India</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Active Report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1970348990198849929</td>\n",
       "      <td>2025-09-23 04:46:21+00:00</td>\n",
       "      <td>BJP is inviting sugg on how  to make Mumbai be...</td>\n",
       "      <td>Sanjeevdesiguy</td>\n",
       "      <td>False</td>\n",
       "      <td>SPACE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.8</td>\n",
       "      <td>Active Report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1970340495512740159</td>\n",
       "      <td>2025-09-23 04:12:35+00:00</td>\n",
       "      <td>@Matt_Pinner Ofcourse yes! The same flood stor...</td>\n",
       "      <td>VedicBeat</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>Active Report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1970327052403515841</td>\n",
       "      <td>2025-09-23 03:19:10+00:00</td>\n",
       "      <td>@motherrr @manjusipayya @dksardana @Ambarseriy...</td>\n",
       "      <td>joydeepg9</td>\n",
       "      <td>False</td>\n",
       "      <td>Gurgaon, India</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Active Report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1970322667883606430</td>\n",
       "      <td>2025-09-23 03:01:45+00:00</td>\n",
       "      <td>@RealDrJaneRuby Literally the most synchronize...</td>\n",
       "      <td>DJohnT1122</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.6</td>\n",
       "      <td>Active Report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1970320416435585122</td>\n",
       "      <td>2025-09-23 02:52:48+00:00</td>\n",
       "      <td>@BaddieDean @hoar27796 @Codex_India3 thanx to ...</td>\n",
       "      <td>jhonbap78</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.6</td>\n",
       "      <td>Active Report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1970319702703439939</td>\n",
       "      <td>2025-09-23 02:49:58+00:00</td>\n",
       "      <td>‚ÄòWorse than Amphan Cyclone‚Äô: Houses flooded, s...</td>\n",
       "      <td>FinancialXpress</td>\n",
       "      <td>False</td>\n",
       "      <td>India</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Active Report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1970319702703439939</td>\n",
       "      <td>2025-09-23 02:49:58+00:00</td>\n",
       "      <td>‚ÄòWorse than Amphan Cyclone‚Äô: Houses flooded, s...</td>\n",
       "      <td>FinancialXpress</td>\n",
       "      <td>False</td>\n",
       "      <td>India</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.6</td>\n",
       "      <td>Active Report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1970300953640280514</td>\n",
       "      <td>2025-09-23 01:35:28+00:00</td>\n",
       "      <td>@influenya @CelticAshes As a Black person, thi...</td>\n",
       "      <td>LeeH63526936</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.6</td>\n",
       "      <td>Active Report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1970291007855210500</td>\n",
       "      <td>2025-09-23 00:55:57+00:00</td>\n",
       "      <td>Cyclone bulletin and #Navratri\\nWeather update...</td>\n",
       "      <td>cyclo_cast_001</td>\n",
       "      <td>False</td>\n",
       "      <td>Palghar, Maharashtra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.6</td>\n",
       "      <td>Active Report</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tweet_id                created_at  \\\n",
       "0   1970351141624902010 2025-09-23 04:54:54+00:00   \n",
       "3   1970350823323627600 2025-09-23 04:53:38+00:00   \n",
       "6   1970350658026185072 2025-09-23 04:52:58+00:00   \n",
       "10  1970348990198849929 2025-09-23 04:46:21+00:00   \n",
       "19  1970340495512740159 2025-09-23 04:12:35+00:00   \n",
       "32  1970327052403515841 2025-09-23 03:19:10+00:00   \n",
       "41  1970322667883606430 2025-09-23 03:01:45+00:00   \n",
       "43  1970320416435585122 2025-09-23 02:52:48+00:00   \n",
       "44  1970319702703439939 2025-09-23 02:49:58+00:00   \n",
       "45  1970319702703439939 2025-09-23 02:49:58+00:00   \n",
       "47  1970300953640280514 2025-09-23 01:35:28+00:00   \n",
       "49  1970291007855210500 2025-09-23 00:55:57+00:00   \n",
       "\n",
       "                                                 text  author_username  \\\n",
       "0   River #Ganga at #Farakka(FF) in #MURSHIDABAD d...   CWCOfficial_FF   \n",
       "3   River #Ganga at #CS 97 A GANGA FARAKKA in #MUR...   CWCOfficial_FF   \n",
       "6   ‚ÄòWorse than Amphan Cyclone‚Äô: Houses flooded, s...      akularsnvas   \n",
       "10  BJP is inviting sugg on how  to make Mumbai be...   Sanjeevdesiguy   \n",
       "19  @Matt_Pinner Ofcourse yes! The same flood stor...        VedicBeat   \n",
       "32  @motherrr @manjusipayya @dksardana @Ambarseriy...        joydeepg9   \n",
       "41  @RealDrJaneRuby Literally the most synchronize...       DJohnT1122   \n",
       "43  @BaddieDean @hoar27796 @Codex_India3 thanx to ...        jhonbap78   \n",
       "44  ‚ÄòWorse than Amphan Cyclone‚Äô: Houses flooded, s...  FinancialXpress   \n",
       "45  ‚ÄòWorse than Amphan Cyclone‚Äô: Houses flooded, s...  FinancialXpress   \n",
       "47  @influenya @CelticAshes As a Black person, thi...     LeeH63526936   \n",
       "49  Cyclone bulletin and #Navratri\\nWeather update...   cyclo_cast_001   \n",
       "\n",
       "    author_verified author_profile_location  tweet_geo is_disaster  \\\n",
       "0             False        New Delhi, India        NaN        True   \n",
       "3             False        New Delhi, India        NaN        True   \n",
       "6             False        Hyderabad, India        NaN        True   \n",
       "10            False                  SPACE         NaN        True   \n",
       "19            False                    None        NaN        True   \n",
       "32            False          Gurgaon, India        NaN        True   \n",
       "41            False                    None        NaN        True   \n",
       "43            False                    None        NaN        True   \n",
       "44            False                   India        NaN        True   \n",
       "45            False                   India        NaN        True   \n",
       "47            False                    None        NaN        True   \n",
       "49            False   Palghar, Maharashtra         NaN        True   \n",
       "\n",
       "    confidence_score    report_type  \n",
       "0                0.8  Active Report  \n",
       "3                0.8  Active Report  \n",
       "6                0.9  Active Report  \n",
       "10               0.8  Active Report  \n",
       "19               0.7  Active Report  \n",
       "32               0.9  Active Report  \n",
       "41               0.6  Active Report  \n",
       "43               0.6  Active Report  \n",
       "44               0.9  Active Report  \n",
       "45               0.6  Active Report  \n",
       "47               0.6  Active Report  \n",
       "49               0.6  Active Report  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verified_disasters_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9c0995",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install spacy geopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc84d1ec",
   "metadata": {},
   "source": [
    "**Location Extraction** - using Named Entity Recognition extracting the Location of the post \n",
    "1. either from the user's tweet\n",
    "2. fall back to the user profile location if tweet location unavailable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46a5d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ spaCy model loaded successfully.\n",
      "‚úÖ GeoPy geocoder initialized.\n",
      "\n",
      "--- üêû Inspecting your DataFrame ---\n",
      "Columns found:\n",
      "Index(['tweet_id', 'created_at', 'text', 'author_username', 'author_verified',\n",
      "       'author_profile_location', 'tweet_geo', 'is_disaster',\n",
      "       'confidence_score', 'report_type'],\n",
      "      dtype='object')\n",
      "----------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üìç Extracting Coordinates: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:13<00:00,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Successfully found coordinates for 11 reports.\n",
      "\n",
      "--- Sample of Final Data with Coordinates ---\n",
      "                                                 text   latitude  longitude\n",
      "0   River #Ganga at #Farakka(FF) in #MURSHIDABAD d...  28.641926  77.221750\n",
      "3   River #Ganga at #CS 97 A GANGA FARAKKA in #MUR...  28.641926  77.221750\n",
      "6   ‚ÄòWorse than Amphan Cyclone‚Äô: Houses flooded, s...  22.572646  88.363895\n",
      "10  BJP is inviting sugg on how  to make Mumbai be...  19.054999  72.869203\n",
      "19  @Matt_Pinner Ofcourse yes! The same flood stor...  22.351115  78.667743\n",
      "\n",
      "üíæ Final geocoded data saved to 'final_geocoded_reports.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from geopy.geocoders import Nominatim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. Setup ---\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"‚úÖ spaCy model loaded successfully.\")\n",
    "except OSError:\n",
    "    print(\"spaCy model not found. Please run: python -m spacy download en_core_web_sm\")\n",
    "    nlp = None\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"sih_disaster_mapper_v4\")\n",
    "print(\"‚úÖ GeoPy geocoder initialized.\")\n",
    "\n",
    "# --- 2. Load Your Classified Data ---\n",
    "\n",
    "# --- DEBUGGING STEP (to confirm) ---\n",
    "print(\"\\n--- üêû Inspecting your DataFrame ---\")\n",
    "print(\"Columns found:\")\n",
    "print(verified_disasters_df.columns)\n",
    "print(\"----------------------------------\\n\")\n",
    "\n",
    "# --- 3. The Location Extraction Function (No changes needed) ---\n",
    "def get_coordinates(text, profile_location):\n",
    "    \"\"\"\n",
    "    Extracts coordinates by first checking the tweet text (NER),\n",
    "    then falling back to the user's profile location.\n",
    "    \"\"\"\n",
    "    location_name = None\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in [\"GPE\", \"LOC\"]:\n",
    "            location_name = ent.text\n",
    "            break\n",
    "            \n",
    "    if not location_name and isinstance(profile_location, str):\n",
    "        location_name = profile_location\n",
    "        \n",
    "    if location_name:\n",
    "        try:\n",
    "            location = geolocator.geocode(location_name, timeout=10)\n",
    "            if location:\n",
    "                return location.latitude, location.longitude\n",
    "        except Exception:\n",
    "            pass\n",
    "            \n",
    "    return None, None\n",
    "\n",
    "# --- 4. Apply Function and Get Final Data ---\n",
    "if nlp and not verified_disasters_df.empty:\n",
    "    tqdm.pandas(desc=\"üìç Extracting Coordinates\")\n",
    "    \n",
    "    # We directly use the 'author_profile_location' column that is already here\n",
    "    verified_disasters_df[['latitude', 'longitude']] = verified_disasters_df.progress_apply(\n",
    "        lambda row: pd.Series(get_coordinates(row['text'], row['author_profile_location'])),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Drop rows where we couldn't find any coordinates\n",
    "    final_geocoded_df = verified_disasters_df.dropna(subset=['latitude', 'longitude']).copy()\n",
    "\n",
    "    print(f\"\\n‚úÖ Successfully found coordinates for {len(final_geocoded_df)} reports.\")\n",
    "    print(\"\\n--- Sample of Final Data with Coordinates ---\")\n",
    "    print(final_geocoded_df[['text', 'latitude', 'longitude']].head())\n",
    "\n",
    "    # --- 5. Save Your Final Progress ---\n",
    "    final_geocoded_df.to_csv('final_geocoded_reports.csv', index=False)\n",
    "    print(\"\\nüíæ Final geocoded data saved to 'final_geocoded_reports.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f82cd06e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>author_username</th>\n",
       "      <th>author_verified</th>\n",
       "      <th>author_profile_location</th>\n",
       "      <th>tweet_geo</th>\n",
       "      <th>is_disaster</th>\n",
       "      <th>confidence_score</th>\n",
       "      <th>report_type</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1970351141624902010</td>\n",
       "      <td>2025-09-23 04:54:54+00:00</td>\n",
       "      <td>River #Ganga at #Farakka(FF) in #MURSHIDABAD d...</td>\n",
       "      <td>CWCOfficial_FF</td>\n",
       "      <td>False</td>\n",
       "      <td>New Delhi, India</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.8</td>\n",
       "      <td>Active Report</td>\n",
       "      <td>28.641926</td>\n",
       "      <td>77.221750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1970350823323627600</td>\n",
       "      <td>2025-09-23 04:53:38+00:00</td>\n",
       "      <td>River #Ganga at #CS 97 A GANGA FARAKKA in #MUR...</td>\n",
       "      <td>CWCOfficial_FF</td>\n",
       "      <td>False</td>\n",
       "      <td>New Delhi, India</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.8</td>\n",
       "      <td>Active Report</td>\n",
       "      <td>28.641926</td>\n",
       "      <td>77.221750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1970350658026185072</td>\n",
       "      <td>2025-09-23 04:52:58+00:00</td>\n",
       "      <td>‚ÄòWorse than Amphan Cyclone‚Äô: Houses flooded, s...</td>\n",
       "      <td>akularsnvas</td>\n",
       "      <td>False</td>\n",
       "      <td>Hyderabad, India</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Active Report</td>\n",
       "      <td>22.572646</td>\n",
       "      <td>88.363895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1970348990198849929</td>\n",
       "      <td>2025-09-23 04:46:21+00:00</td>\n",
       "      <td>BJP is inviting sugg on how  to make Mumbai be...</td>\n",
       "      <td>Sanjeevdesiguy</td>\n",
       "      <td>False</td>\n",
       "      <td>SPACE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.8</td>\n",
       "      <td>Active Report</td>\n",
       "      <td>19.054999</td>\n",
       "      <td>72.869203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1970340495512740159</td>\n",
       "      <td>2025-09-23 04:12:35+00:00</td>\n",
       "      <td>@Matt_Pinner Ofcourse yes! The same flood stor...</td>\n",
       "      <td>VedicBeat</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>Active Report</td>\n",
       "      <td>22.351115</td>\n",
       "      <td>78.667743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tweet_id                created_at  \\\n",
       "0   1970351141624902010 2025-09-23 04:54:54+00:00   \n",
       "3   1970350823323627600 2025-09-23 04:53:38+00:00   \n",
       "6   1970350658026185072 2025-09-23 04:52:58+00:00   \n",
       "10  1970348990198849929 2025-09-23 04:46:21+00:00   \n",
       "19  1970340495512740159 2025-09-23 04:12:35+00:00   \n",
       "\n",
       "                                                 text author_username  \\\n",
       "0   River #Ganga at #Farakka(FF) in #MURSHIDABAD d...  CWCOfficial_FF   \n",
       "3   River #Ganga at #CS 97 A GANGA FARAKKA in #MUR...  CWCOfficial_FF   \n",
       "6   ‚ÄòWorse than Amphan Cyclone‚Äô: Houses flooded, s...     akularsnvas   \n",
       "10  BJP is inviting sugg on how  to make Mumbai be...  Sanjeevdesiguy   \n",
       "19  @Matt_Pinner Ofcourse yes! The same flood stor...       VedicBeat   \n",
       "\n",
       "    author_verified author_profile_location  tweet_geo is_disaster  \\\n",
       "0             False        New Delhi, India        NaN        True   \n",
       "3             False        New Delhi, India        NaN        True   \n",
       "6             False        Hyderabad, India        NaN        True   \n",
       "10            False                  SPACE         NaN        True   \n",
       "19            False                    None        NaN        True   \n",
       "\n",
       "    confidence_score    report_type   latitude  longitude  \n",
       "0                0.8  Active Report  28.641926  77.221750  \n",
       "3                0.8  Active Report  28.641926  77.221750  \n",
       "6                0.9  Active Report  22.572646  88.363895  \n",
       "10               0.8  Active Report  19.054999  72.869203  \n",
       "19               0.7  Active Report  22.351115  78.667743  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verified_disasters_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d40b372",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install h3 folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2e2e0508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully loaded 11 geocoded reports.\n",
      "‚úÖ Prepared hexagon boundaries for mapping.\n",
      "\n",
      "üó∫Ô∏è  Success! Your ENHANCED hotspot map has been saved to 'disaster_hotspot_map_enhanced.html'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import h3\n",
    "import folium\n",
    "\n",
    "# --- 1. Load Your Final Geocoded Data ---\n",
    "try:\n",
    "    df = pd.read_csv('final_geocoded_reports.csv')\n",
    "    print(f\"‚úÖ Successfully loaded {len(df)} geocoded reports.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"üõë Error: 'final_geocoded_reports.csv' not found.\")\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "if not df.empty:\n",
    "    # --- 2. H3 Conversion and Density Calculation (No changes here) ---\n",
    "    H3_RESOLUTION = 7\n",
    "    df['h3_index'] = df.apply(\n",
    "        lambda row: h3.latlng_to_cell(row['latitude'], row['longitude'], H3_RESOLUTION),\n",
    "        axis=1\n",
    "    )\n",
    "    report_density = df['h3_index'].value_counts().reset_index()\n",
    "    report_density.columns = ['h3_index', 'report_count']\n",
    "\n",
    "    # --- 3. Prepare Hexagons for Visualization (No changes here) ---\n",
    "    def get_hexagon_boundary(h3_index):\n",
    "        boundary = h3.cell_to_boundary(h3_index)\n",
    "        return [[lon, lat] for lat, lon in boundary]\n",
    "    \n",
    "    report_density['hexagon'] = report_density['h3_index'].apply(get_hexagon_boundary)\n",
    "    print(\"‚úÖ Prepared hexagon boundaries for mapping.\")\n",
    "\n",
    "    # --- 4. Visualize the Hotspot Map with Enhanced Styling ---\n",
    "\n",
    "    # --- NEW: A function to determine color based on report count ---\n",
    "    def get_fill_color(count):\n",
    "        max_count = report_density['report_count'].max()\n",
    "        if count > max_count * 0.66:\n",
    "            return '#d73027' # Dark Red for high density\n",
    "        elif count > max_count * 0.33:\n",
    "            return '#fc8d59' # Orange for medium density\n",
    "        else:\n",
    "            return '#fee08b' # Yellow for low density\n",
    "\n",
    "    map_center = [df['latitude'].mean(), df['longitude'].mean()]\n",
    "    hotspot_map = folium.Map(location=map_center, zoom_start=10, tiles=\"cartodbpositron\")\n",
    "\n",
    "    for i, row in report_density.iterrows():\n",
    "        folium.GeoJson(\n",
    "            data={\n",
    "                \"type\": \"Feature\",\n",
    "                \"geometry\": {\n",
    "                    \"type\": \"Polygon\",\n",
    "                    \"coordinates\": [row['hexagon']],\n",
    "                },\n",
    "            },\n",
    "            # --- NEW: Updated style function ---\n",
    "            style_function=lambda feature, count=row['report_count']: {\n",
    "                'fillColor': get_fill_color(count), # Use our new color function\n",
    "                'color': 'white',      # A high-contrast white border\n",
    "                'weight': 2.5,         # Make the border thicker\n",
    "                'fillOpacity': 0.75,     # A consistent, strong opacity\n",
    "            },\n",
    "            tooltip=f\"<b>üî• Hotspot üî•</b><br>Reports: {row['report_count']}<br>H3 Index: {row['h3_index']}\"\n",
    "        ).add_to(hotspot_map)\n",
    "\n",
    "    hotspot_map.save(\"disaster_hotspot_map_enhanced.html\")\n",
    "    print(\"\\nüó∫Ô∏è  Success! Your ENHANCED hotspot map has been saved to 'disaster_hotspot_map_enhanced.html'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c82c7b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully loaded 11 geocoded reports.\n",
      "‚úÖ Prepared hexagon data for mapping.\n",
      "üõ∞Ô∏è  Added Satellite base layer.\n",
      "üß≠ Added clickable index and layer control.\n",
      "\n",
      "üó∫Ô∏è  Success! Your new INTERACTIVE map has been saved to 'disaster_hotspot_map_interactive.html'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import h3\n",
    "import folium\n",
    "from folium.features import MacroElement\n",
    "from jinja2 import Template\n",
    "\n",
    "# --- 1. Load Your Final Geocoded Data ---\n",
    "try:\n",
    "    df = pd.read_csv('final_geocoded_reports.csv')\n",
    "    print(f\"‚úÖ Successfully loaded {len(df)} geocoded reports.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"üõë Error: 'final_geocoded_reports.csv' not found.\")\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "if not df.empty:\n",
    "    # --- 2. H3 Conversion and Density Calculation ---\n",
    "    H3_RESOLUTION = 7\n",
    "    df['h3_index'] = df.apply(\n",
    "        lambda row: h3.latlng_to_cell(row['latitude'], row['longitude'], H3_RESOLUTION),\n",
    "        axis=1\n",
    "    )\n",
    "    report_density = df['h3_index'].value_counts().reset_index()\n",
    "    report_density.columns = ['h3_index', 'report_count']\n",
    "\n",
    "    # --- 3. Prepare Hexagons and Get Center Coordinates ---\n",
    "    def get_hexagon_data(h3_index):\n",
    "        boundary = h3.cell_to_boundary(h3_index)\n",
    "        center = h3.cell_to_latlng(h3_index)\n",
    "        return [[lon, lat] for lat, lon in boundary], center\n",
    "\n",
    "    # Apply the function to get both boundary and center\n",
    "    report_density[['hexagon', 'center']] = report_density['h3_index'].apply(\n",
    "        lambda x: pd.Series(get_hexagon_data(x))\n",
    "    )\n",
    "    print(\"‚úÖ Prepared hexagon data for mapping.\")\n",
    "\n",
    "    # --- 4. Create the Map with Satellite View ---\n",
    "    map_center = [df['latitude'].mean(), df['longitude'].mean()]\n",
    "    \n",
    "    # Initialize the map with a standard view\n",
    "    hotspot_map = folium.Map(location=map_center, zoom_start=10, tiles=\"OpenStreetMap\")\n",
    "\n",
    "    # Add the satellite tile layer\n",
    "    folium.TileLayer(\n",
    "        tiles='https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}',\n",
    "        attr='Esri',\n",
    "        name='Satellite View',\n",
    "        overlay=False,\n",
    "        control=True\n",
    "    ).add_to(hotspot_map)\n",
    "    \n",
    "    print(\"üõ∞Ô∏è  Added Satellite base layer.\")\n",
    "\n",
    "    # --- 5. Draw the Hexagonal Hotspots (no changes here) ---\n",
    "    # (Your existing code for styling and drawing hexagons)\n",
    "    def get_fill_color(count):\n",
    "        max_count = report_density['report_count'].max()\n",
    "        if count > max_count * 0.66: return '#d73027'\n",
    "        elif count > max_count * 0.33: return '#fc8d59'\n",
    "        else: return '#fee08b'\n",
    "\n",
    "    for i, row in report_density.iterrows():\n",
    "        folium.GeoJson(\n",
    "            data={\"type\": \"Feature\", \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [row['hexagon']]}},\n",
    "            style_function=lambda feature, count=row['report_count']: {\n",
    "                'fillColor': get_fill_color(count), 'color': 'white', 'weight': 2.5, 'fillOpacity': 0.75,\n",
    "            },\n",
    "            tooltip=f\"<b>üî• Hotspot üî•</b><br>Reports: {row['report_count']}<br>H3 Index: {row['h3_index']}\"\n",
    "        ).add_to(hotspot_map)\n",
    "\n",
    "    # --- 6. Create and Add the Clickable Index ---\n",
    "    # We use Jinja2 to create a custom HTML/CSS/JS element\n",
    "    template = \"\"\"\n",
    "    {% macro html(this, kwargs) %}\n",
    "    <div id=\"maplegend\" class=\"leaflet-control leaflet-bar\" \n",
    "         style=\"position: fixed; bottom: 20px; right: 20px; z-index:9999;\n",
    "                background-color: rgba(255, 255, 255, 0.8);\n",
    "                border-radius: 5px; padding: 10px; font-size:12px;\n",
    "                max-height: 200px; overflow-y: auto;\n",
    "                border: 2px solid grey;\">\n",
    "    <h4 style=\"margin-top:0;\">Hotspot Zones</h4>\n",
    "    <ul style=\"list-style-type:none; padding-left:0;\">\n",
    "    {% for i, row in this.hotspots.iterrows() %}\n",
    "        <li style=\"margin-bottom: 5px;\">\n",
    "            <a href=\"#\" style=\"text-decoration: none; color: black;\" \n",
    "               onclick=\"\n",
    "                   L.DomEvent.preventDefault(event);\n",
    "                   {{this._parent.get_name()}}.setView([{{row.center[0]}}, {{row.center[1]}}], 13);\n",
    "                   return false;\n",
    "               \">\n",
    "               Hotspot {{i+1}} ({{row.report_count}} reports)\n",
    "            </a>\n",
    "        </li>\n",
    "    {% endfor %}\n",
    "    </ul>\n",
    "    </div>\n",
    "    {% endmacro %}\n",
    "    \"\"\"\n",
    "    macro = MacroElement()\n",
    "    macro._template = Template(template)\n",
    "    # Pass the hotspot data from Python to the HTML template\n",
    "    macro.hotspots = report_density.sort_values(by='report_count', ascending=False)\n",
    "    hotspot_map.add_child(macro)\n",
    "    \n",
    "    # Add a layer control to switch between maps\n",
    "    folium.LayerControl().add_to(hotspot_map)\n",
    "    print(\"üß≠ Added clickable index and layer control.\")\n",
    "\n",
    "    # --- 7. Save the Final Map ---\n",
    "    hotspot_map.save(\"disaster_hotspot_map_interactive.html\")\n",
    "    print(\"\\nüó∫Ô∏è  Success! Your new INTERACTIVE map has been saved to 'disaster_hotspot_map_interactive.html'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dcc0528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from ddgs.ddgs import DDGS\n",
    "from PIL import Image\n",
    "\n",
    "# The main media directory\n",
    "MEDIA_DIR = \"media\"\n",
    "# The specific subdirectory for downloaded images, as per your structure\n",
    "VISUALS_DIR = os.path.join(MEDIA_DIR, \"visuals\")\n",
    "\n",
    "def get_media_visuals(keywords, location, num_images=5):\n",
    "    \"\"\"\n",
    "    Searches for and downloads relevant visuals into the 'media/visuals' directory.\n",
    "    This function is designed to be called from your core.ipynb notebook.\n",
    "    \"\"\"\n",
    "    # Create the visuals subdirectory if it doesn't exist\n",
    "    if not os.path.exists(VISUALS_DIR):\n",
    "        os.makedirs(VISUALS_DIR)\n",
    "\n",
    "    # Clean up keywords and create a search query\n",
    "    search_keywords = ' '.join(keywords) if isinstance(keywords, list) else keywords\n",
    "    search_query = f\"{search_keywords} {location}\"\n",
    "    print(f\"üé• Searching for visuals with query: '{search_query}'...\")\n",
    "    downloaded_images = []\n",
    "    \n",
    "    try:\n",
    "        with DDGS() as ddgs:\n",
    "            results = ddgs.images(\n",
    "                search_query,\n",
    "                region=\"in-en\",\n",
    "                safesearch=\"on\",\n",
    "                size=\"Large\",\n",
    "                type_image=\"photo\"\n",
    "            )\n",
    "            \n",
    "            if not results:\n",
    "                print(f\"üõë No images found for '{search_query}'.\")\n",
    "                return []\n",
    "\n",
    "            print(f\"Downloading up to {num_images} images...\")\n",
    "            # Create a unique prefix for filenames based on the location\n",
    "            location_prefix = location.split(',')[0].lower().replace(' ', '_')\n",
    "            for i, r in enumerate(results):\n",
    "                if i >= num_images:\n",
    "                    break\n",
    "                try:\n",
    "                    image_url = r.get(\"image\")\n",
    "                    if not image_url:\n",
    "                        continue\n",
    "                    \n",
    "                    image_data = requests.get(image_url, timeout=15).content\n",
    "                    # Save the file with the location prefix for easy lookup later\n",
    "                    filename = os.path.join(VISUALS_DIR, f\"{location_prefix}_image_{i}.jpg\")\n",
    "                    with open(filename, 'wb') as f:\n",
    "                        f.write(image_data)\n",
    "                    Image.open(filename).verify()\n",
    "                    downloaded_images.append(filename)\n",
    "                except Exception as e:\n",
    "                    print(f\"  - Could not download or verify an image: {e}\")\n",
    "                    \n",
    "        print(f\"‚úÖ Downloaded {len(downloaded_images)} valid images for {location}.\")\n",
    "        return downloaded_images\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"üõë Error fetching visuals: {e}\")\n",
    "        return []\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
